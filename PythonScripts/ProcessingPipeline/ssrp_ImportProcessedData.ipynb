{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np \n",
    "#import scipy \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt #import matplotlib as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns #import mat73\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "import mne\n",
    "from mne.viz import plot_topomap\n",
    "from mne.io import RawArray\n",
    "#import numpy.matlib\n",
    "import scipy.stats as stats \n",
    "from scipy.stats import chi2, f\n",
    "from scipy.stats import f as f_dist\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.io import savemat\n",
    "sns.set_theme() # set the plotting atmosphere\n",
    "rbow = sns.color_palette('husl',8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Basic Global Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "NumHarms = 4\n",
    "NumBins = 6 # 6 contrasts\n",
    "NumChans = 128\n",
    "NumHemis = 2 # hemifield data = NumHarms*NumHemis # 8 : 2f1 4f1 6f1 8f1 2f2 4f2 6f2 8f2\n",
    "NumFriters = NumHarms*NumHemis\n",
    "chanInd = np.arange(1,NumChans,1) # channel index ..\n",
    "cll = ['1%','3%','5%','16%','40%','100%'] # contrast levels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Functions to get average complex values per subj per cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def AvgComplexVals(NumSubs,NumBins,NumChans,RealImaginary,xfxiter,ppi):\n",
    "    '''This function takes one key 2F1-8F2 AND\n",
    "    only a PRE OR POST data\n",
    "    RETURNS: [2X6X128] ARRAY \n",
    "    [0] = real / imaginary values\n",
    "    [1] = 1% - 100% sweep (6) data\n",
    "    [2] = all eeg channels\n",
    "    '''\n",
    "    pp_mat = np.zeros((NumSubs,NumBins,2,NumChans))\n",
    "    # importing all pre data , real and imarinary numbers\n",
    "    for si in range(NumSubs): # NumSubs\n",
    "        # data in is PrePost x Contrast x real/imaginary x eeg channels\n",
    "        xfx_d =  RealImaginary[si][xfxiter][ppi,:,:,:]\n",
    "        #print(xfx_d.shape)\n",
    "        pp_mat[si,:,:,:] = xfx_d\n",
    "    complex_avgs = np.zeros((2,NumBins,NumChans))\n",
    "    for i in range(2):\n",
    "        complex_avgs[i] = np.nanmean(pp_mat[:,:,i,:],axis = 0) # [real imaginary x contrast x channel]\n",
    "    return complex_avgs\n",
    "\n",
    "### good function - might have to toss the above \n",
    "def GetComplexValsPerSubj(NumSubs,NumChans,RealImaginary,xfi,contrast_val):\n",
    "    ''''\n",
    "    This function iterates for:\n",
    "    ONE XFX DATA KEY AND GETS ALL THE PRE POST COMPLEX VALUES FOR ALL \n",
    "    PARTICIPANTS FOR ONE CONTRAST FOR EACH CHANNEL.\n",
    "    RETURNS 2 keys [PRE/POST]: each with 3D ARRAY : [NumSubs x Real/Imaginary x Channels]\n",
    "\n",
    "    xfi = 0-8 any 2F1 - 8F2 data \n",
    "    contrast_val = Any value 0-5 [increasing contrast]\n",
    "    '''\n",
    "    prepost_complexs = {}\n",
    "\n",
    "    for prepost in range(2):\n",
    "        prepost_complexs[prepost] = {} # init pre == 0 and post  == 1 subkeys\n",
    "        getcomplexs = np.zeros((NumSubs,2,NumChans)) # get each subjects complexs for each channel [2x128]\n",
    "        for sIn in range(NumSubs):\n",
    "            #complexIn = RealImaginary[sIn][xfxiter][prepost,contrast_val,:,:] # [real/imaginary vals / channels]\n",
    "            getcomplexs[sIn,:]= RealImaginary[sIn][xfi][prepost,contrast_val,:,:]\n",
    "        prepost_complexs[prepost] = getcomplexs\n",
    "\n",
    "    return prepost_complexs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init MNE + Topo Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# montage info \n",
    "montageIn = mne.channels.make_standard_montage(\"GSN-HydroCel-128\")\n",
    "info = mne.create_info(ch_names=montageIn.ch_names, sfreq=1000, ch_types=\"eeg\")\n",
    "info.set_montage(montageIn)\n",
    "print(montageIn)\n",
    "\n",
    "# function to plot topos\n",
    "def init_TopoTemplate(title_In):\n",
    "    \"\"\"Prep topo plot fig and axes\"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(18,4), gridspec_kw=dict(height_ratios=[4]))\n",
    "    #axes.axis('off')\n",
    "    #axes[1].axis('off')\n",
    "    #axes.gridspec_kw=dict(height_ratios=[3])\n",
    "    #axes[1].gridspec_kw=dict(height_ratios=[3])\n",
    "    #plt.tight_layout()\n",
    "    plt.suptitle(f'{title_In}', fontsize = 20)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Stats Functions from SVNDL Lab  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def mbox(data1, data2, alphaVal):\n",
    "    # Get the number of samples in each dataset\n",
    "    n1 = data1.shape[0]\n",
    "    n2 = data2.shape[0]\n",
    "    \n",
    "    # Calculate covariance matrices for each dataset\n",
    "    S1 = np.cov(data1, rowvar=False)\n",
    "    S2 = np.cov(data2, rowvar=False)\n",
    "    \n",
    "    # Get the number of columns (features) and groups\n",
    "    p = data1.shape[1]  # Assuming data1 and data2 have the same number of features\n",
    "    g = 2  # Assuming two groups\n",
    "    \n",
    "    # Compute pooled covariance matrix\n",
    "    S12 = ((n1 - 1) * S1 + (n2 - 1) * S2) / (n1 + n2 - 2)\n",
    "    \n",
    "    # Compute F statistic components\n",
    "    Fstat = (n1 - 1) * np.log(np.linalg.det(S1)) + (n2 - 1) * np.log(np.linalg.det(S2))\n",
    "    MB = (n1 + n2 - 2) * np.log(np.linalg.det(S12)) - Fstat\n",
    "    \n",
    "    # Calculate correction factor\n",
    "    sum1 = 1 / (n1 - 1) + 1 / (n2 - 1)\n",
    "    sum2 = 1 / ((n1 - 1) ** 2) + 1 / ((n2 - 1) ** 2)\n",
    "    cFactor = (((2 * p ** 2) + 3 * p - 1) / (6 * (p + 1) * (g - 1))) * (sum1 - 1 / (n1 + n2 - 2))\n",
    "    \n",
    "    # Determine which approximation to use based on sample size\n",
    "    bw_cutoff = 20\n",
    "    useApprox = 'Chi2' if n1 > bw_cutoff or n2 > bw_cutoff else 'F'\n",
    "    \n",
    "    if useApprox == 'Chi2':\n",
    "        # Chi-square approximation\n",
    "        X2 = MB * (1 - cFactor)\n",
    "        v = (p * (p + 1) * (g - 1)) // 2  # Degrees of freedom\n",
    "        pVal = 1 - chi2.cdf(X2, v)\n",
    "    else:\n",
    "        # F approximation\n",
    "        C0 = (sum2 - 1 / (n1 + n2 - 2) ** 2) * (p - 1) * (p + 2) / (6 * (g - 1))\n",
    "        cDiff = C0 - cFactor ** 2\n",
    "        v1 = p * (p + 1) * (g - 1) // 2  # Numerator degrees of freedom\n",
    "        \n",
    "        if cDiff >= 0:\n",
    "            v21 = int((v1 + 2) / cDiff)  # Denominator degrees of freedom\n",
    "            Fstat = MB * (1 - cFactor - (v1 / v21)) / v1  # F approximation\n",
    "            pVal = 1 - f.cdf(Fstat, v1, v21)  # Significance value associated with the observed F statistic\n",
    "        else:\n",
    "            v22 = int(-(v1 + 2) / cDiff)  # Denominator degrees of freedom\n",
    "            b = v22 / (1 - cFactor - 2 / v22)\n",
    "            Fstat = (v22 * MB) / (v1 * (b - MB))  # F approximation\n",
    "            pVal = 1 - f.cdf(Fstat, v1, v22)  # Significance value associated with the observed F statistic\n",
    "    # Determine if there is a significant difference based on alpha level\n",
    "    sig_diff = 0\n",
    "    if pVal < alphaVal:\n",
    "        sig_diff = 1\n",
    "    \n",
    "    return sig_diff, pVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def eigFourierCoefs(xyData):\n",
    "    # Get dimensions of xyData\n",
    "    dims = xyData.shape\n",
    "    N = dims[0]\n",
    "    \n",
    "    # Check input data dimensions\n",
    "    if len(dims) != 2 or dims[1] != 2:\n",
    "        raise ValueError('Input data must be a matrix of 2D row samples')\n",
    "    if N < 2:\n",
    "        raise ValueError('Input data must contain at least 2 samples')\n",
    "    \n",
    "    srIx = 0  # Index for real coefficients in xyData\n",
    "    siIx = 1  # Index for imaginary coefficients in xyData\n",
    "    \n",
    "    sampMu = np.mean(xyData, axis=0)\n",
    "    sampCovMat = np.cov(xyData.T)  # Transpose xyData to get correct covariance matrix\n",
    "    \n",
    "    eigenval, eigenvec = np.linalg.eig(sampCovMat)\n",
    "    \n",
    "    # Sort the eigenvectors by their eigenvalues\n",
    "    eigAscendIx = np.argsort(eigenval)\n",
    "    smaller_eigenvec = eigenvec[:, eigAscendIx[0]]\n",
    "    larger_eigenvec = eigenvec[:, eigAscendIx[1]]\n",
    "    smaller_eigenval = eigenval[eigAscendIx[0]]\n",
    "    larger_eigenval = eigenval[eigAscendIx[1]]\n",
    "    \n",
    "    phi = np.arctan2(larger_eigenvec[siIx], larger_eigenvec[srIx])\n",
    "    # Ensure phi is in the range [0, 2*pi]\n",
    "    phi = (phi + 2 * np.pi) % (2 * np.pi)\n",
    "    \n",
    "    return sampMu, sampCovMat, smaller_eigenvec, smaller_eigenval, larger_eigenvec, larger_eigenval, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def t2FC(xyData1, xyData2, testMu=[0, 0], alphaVal=0.05):\n",
    "    \"\"\"Returns the results of running Hotelling's t-squared test that the mean\n",
    "    % of the 2D data in xyData is the same as the mean specified in testMu at\n",
    "    % the specified alphaVal (0-1).\n",
    "    %\n",
    "    % Based on Anderson (1984) An Introduction to Multivariate Statistical \n",
    "    % Analysis, Wiley\n",
    "    %\n",
    "    % In:\n",
    "    %   xyData - n x 2 x q matrix containing n data samples\n",
    "    %            if q = 1, data will be tested against zero    \n",
    "    %            if q = 2, a paired t-test will be performed,\n",
    "    %            testing xyData(:,:,1) against xyData(:,:,2). \n",
    "    %            2 is the maximum length of the third dimension. \n",
    "    %            Function assumes that the 2D data in xyData(:,:,1)\n",
    "    %            and the optional xyData(:,:,2) are Fourier \n",
    "    %            coefficients organized with the real coefficients \n",
    "    %            in the 1st column and the imaginary\n",
    "    %            coefficients in the 2nd column. Rows = samples.\n",
    "    % \n",
    "    % <options>:\n",
    "    %   testMu - 2-element vector specifying the mean to test against ([0,0]) \n",
    "    % \n",
    "    %   alphaVal - scalar indicating the alpha value for testing ([0.05])\n",
    "    %\n",
    "    %   pdStyle - do PowerDiva style testing (true/[false])\n",
    "    %\n",
    "    % Out:\n",
    "    %\n",
    "    %   results - struct containing the following fields:\n",
    "    %             alpha, tSqrdCritical, tSqrd, pVal, H (0 if can't reject null; 1 if\n",
    "    %             rejected the null hypothesis)\"\"\"\n",
    "    # Check input data dimensions\n",
    "    if xyData1.shape[0] < 2 or xyData2.shape[0] < 2:\n",
    "        raise ValueError('Input data must contain at least 2 samples')\n",
    "    if xyData1.shape[1] != 2 or xyData2.shape[1] != 2:\n",
    "        print('Warning: Input data must be a matrix of 2D row samples')\n",
    "\n",
    "    # Add a third dimension if not provided\n",
    "    if len(xyData1.shape) < 3:\n",
    "        #print('Data input is 2D, now adding selected 3rd dimension')\n",
    "        xy1_zeros_3d = np.zeros((xyData1.shape[0], xyData1.shape[1], 2))\n",
    "        xy2_zeros_3d = np.zeros((xyData2.shape[0], xyData2.shape[1], 2))\n",
    "\n",
    "        for shove in range(2):\n",
    "            if shove == 0:\n",
    "                xy1_zeros_3d[:,:,shove] = xyData1\n",
    "                xy2_zeros_3d[:,:,shove] = xyData2\n",
    "            if shove == 1:\n",
    "                xy1_zeros_3d[:,:,shove] = np.zeros_like(xyData1)\n",
    "                xy2_zeros_3d[:,:,shove] = np.zeros_like(xyData2)\n",
    "\n",
    "        xyD1 = xy1_zeros_3d\n",
    "        xyD2 = xy2_zeros_3d\n",
    "\n",
    "        # xyData1 = np.concatenate((xyData1, np.zeros(xyData1.shape)), axis=2)\n",
    "        # xyData2 = np.concatenate((xyData2, np.zeros(xyData2.shape)), axis=2)\n",
    "    elif len(xyData1.shape) > 2:\n",
    "        print('Warning: Length of third dimension of input data may not exceed two')\n",
    "        xyD1 = xyData1\n",
    "        xyD2 = xyData2\n",
    "\n",
    "    # Calculate differences\n",
    "    d1 = xyD1[:, :, 0] - xyD1[:, :, 1] # \n",
    "    d2 = xyD2[:, :, 0] - xyD2[:, :, 1]\n",
    "\n",
    "    dData1 = d1[~np.any(np.isnan(d1), axis=1)]\n",
    "    dData2 = d2[~np.any(np.isnan(d2), axis=1)]\n",
    "\n",
    "    n1 = dData1.shape[0]\n",
    "    n2 = dData2.shape[0]\n",
    "\n",
    "    try:\n",
    "        sMu1, sCov1, _, _, _, _, _ = eigFourierCoefs(dData1)\n",
    "        sMu2, sCov2, _, _, _, _, _ = eigFourierCoefs(dData2)\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        print('The covariance matrix of xyData could not be calculated, probably your data do not contain >1 sample.')\n",
    "        sCov1 = np.cov(dData1.T)\n",
    "        sCov2 = np.cov(dData2.T)\n",
    "        sMu1 = np.mean(dData1, axis=0)\n",
    "        sMu2 = np.mean(dData2, axis=0)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    results['alpha'] = alphaVal\n",
    "    p = xyData1.shape[1]\n",
    "    df1 = p\n",
    "    df2 = n1 + n2 - p - 1\n",
    "    # calculate critical value \n",
    "    t0Sqrd = (((n1 + n2 - 2) * p) / df2) * f.ppf(1 - alphaVal, df1, df2)\n",
    "    results['tSqrdCritical'] = t0Sqrd\n",
    "\n",
    "    # Perform M-Box test\n",
    "    alphaValMBox = 0.01\n",
    "    sig_diff, pValMBox = mbox(dData1, dData2, alphaValMBox)\n",
    "    diff_mu = sMu1 - sMu2\n",
    "\n",
    "    if not sig_diff:  # If covariance matrices are not significantly different\n",
    "        b = 0 # get index for sig diff \n",
    "        print(\"Matrices not significantly different\")\n",
    "        S_pool = ((n1 - 1) * sCov1 + (n2 - 1) * sCov2) / (n1 - 1 + n2 - 1)\n",
    "        diff_S_pool = S_pool * (1 / n1 + 1 / n2)\n",
    "        invCovMat = np.linalg.inv(diff_S_pool)\n",
    "        results['tSqrd'] = np.dot(np.dot(diff_mu - testMu, invCovMat), (diff_mu - testMu).T)\n",
    "        tSqrdF = results['tSqrd'] * (df2 / (df1 * (df2 + 1)))\n",
    "        results['pVal'] = 1 - f.cdf(tSqrdF, df1, df2)\n",
    "    else:  # Unequal covariance matrices\n",
    "        b = 1 # get simple index for sig diff yes or no\n",
    "        print(\"Matrices significantly different\")\n",
    "        S_pool = sCov1 / n1 + sCov2 / n2\n",
    "        diff_mu = sMu1 - sMu2\n",
    "        invCovMat = np.linalg.inv(S_pool)\n",
    "        results['tSqrd'] = np.dot(np.dot(diff_mu - testMu, invCovMat), (diff_mu - testMu).T)\n",
    "        tSqrdF = results['tSqrd'] * (n1 + n2 - 1 - p) / (p * (n1 - 1 + n2 - 1))\n",
    "        df1 = p\n",
    "        s1 = np.dot(np.dot(diff_mu - testMu, invCovMat), sCov1 / n1)\n",
    "        s2 = np.dot(np.dot(diff_mu - testMu, invCovMat), sCov2 / n2)\n",
    "        df2 = results['tSqrd']**2 / (s1**2 / (n1 - 1) + s2**2 / (n2 - 1))\n",
    "        F_Crit = f.ppf(alphaVal, df1, df2)\n",
    "        results['pVal'] = 1 - f.cdf(tSqrdF, df1, df2)\n",
    "\n",
    "    results['H'] = tSqrdF >= results['tSqrdCritical']\n",
    "\n",
    "    return results, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# def GetReal_Imaginary_Data(NumBins,NumChans,RealImaginary,sIn, harmonicInd):\n",
    "#     # store all values per contrast\n",
    "#     ComplexValuesPerBin = np.zeros((NumBins,2,2,NumChans)) # Contrast x Pre/Post x Real/Imaginary x 128 Channels\n",
    "\n",
    "#     fIn = RealImaginary[sIn][harmonicInd]\n",
    "#     condition_split = int((fIn.shape[3])/ 2)\n",
    "\n",
    "#     for c in range(NumBins):\n",
    "#         # pre post index\n",
    "#         pre = fIn[c,:,:,:condition_split] # lowest contrast at 1%, \n",
    "#         post = fIn[c,:,:,condition_split:] \n",
    "#         pre_post_set = [pre,post] # group pre / post data for loop\n",
    "#         complexVals = np.zeros((2,2,(NumChans))) # pre/post x real/imaginary x all channels\n",
    "#         # average real and imaginary values per single channel for respective trials\n",
    "#         for ppsi in range(len(pre_post_set)): # pre post set index\n",
    "#             dsIn= pre_post_set[ppsi] # import pre or post \n",
    "#             for ch in range((NumChans)):\n",
    "#                 realData = dsIn[0,ch,:] # index real values per 1 channel for all trials\n",
    "#                 imagData = dsIn[1,ch,:] # index imaginary values per 1 channel for all trials\n",
    "#                 r_avg = np.nanmean(realData) # returns 1 values per channel\n",
    "#                 i_avg = np.nanmean(imagData) # returns 1 value per channel\n",
    "#                 complexVals[ppsi,0,ch] = r_avg\n",
    "#                 complexVals[ppsi,1,ch] = i_avg\n",
    "                \n",
    "#                 ComplexValuesPerBin[c,:,:,:] = complexVals\n",
    "#     return ComplexValuesPerBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# def ConcateAllDataperCo(NumSubs,AttnX_bool,GoodSubjArr,xFxData,xFX_Ind):\n",
    "#     \"\"\"Concates all subj data into a 4d array\n",
    "#         DictIn = Dict of all data \n",
    "#         AttnX_bool and GoodSubjArr = binary bools to sort data\n",
    "#         xFx_Ind = keys for all haronic and hemifield data\n",
    "#         Returns::: Dict with 2 keys (attnL and attnR conditions given xFx)\"\"\"\n",
    "#     AllData_perSubjCo = {}\n",
    "#     co_temp1 = []\n",
    "#     co_temp2 = []\n",
    "#     for suIn in range(NumSubs):\n",
    "#         if AttnX_bool[suIn] and GoodSubjArr[suIn] == 1:\n",
    "#             co_temp1.append(xFxData[suIn][xFX_Ind])\n",
    "\n",
    "#         if AttnX_bool[suIn] == 0 and GoodSubjArr[suIn] == 1:\n",
    "#             co_temp2.append(xFxData[suIn][xFX_Ind])\n",
    "\n",
    "#     AllData_perSubjCo[0] = np.array(co_temp1) # all attnL data\n",
    "#     AllData_perSubjCo[1] = np.array(co_temp2) # all attnR data\n",
    "#     return AllData_perSubjCo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Main Directory of processed file from MatLab\n",
    "MainDir = 'C:\\\\plimon\\\\LTP_analysis\\\\eegMatFiles\\\\AllMat' # set dir\n",
    "os.chdir(MainDir) # change old dir, to this dir\n",
    "dataFileNames = os.listdir(MainDir) # list files in dir\n",
    "print(f'Total Files Avilable: {len(dataFileNames)}')\n",
    "##############################################\n",
    "FileN = dataFileNames[-1]# choose one                        \n",
    "file_path1 = os.path.join(MainDir, FileN) # join paths and prep 2 load\n",
    "print('Current WD:',file_path1) # does path exist ... ?\n",
    "print('Does File #1 Exist?',os.path.exists(file_path1)) # yes or no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data from Imported pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "loadData = pkl.load(open(file_path1, 'rb'))\n",
    "#print(loadData.keys())\n",
    "xFxData = loadData['EEGData']\n",
    "FileNames = loadData['SubIDs']\n",
    "RealImaginary = loadData['RealImaginaryData_sIns']\n",
    "NumSubs = int(len(RealImaginary.keys()))\n",
    "print(f'Total Subjects : {NumSubs}')\n",
    "\n",
    "str_catch = '-'\n",
    "SubjID = np.array([file_name.split(str_catch)[1] for file_name in FileNames])\n",
    "AttnX_Condition = np.array([file_name.split(str_catch)[2] for file_name in FileNames])\n",
    "UniqueSubjs, SessionsComp = np.unique(SubjID, return_counts = True)\n",
    "print(f'{int(len(UniqueSubjs))} Total Subjects')\n",
    "print(f'{np.sum(SessionsComp == 2)} subjects completed both sessions')\n",
    "print(SubjID) # subject ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AttnX_bool = np.zeros((NumSubs))\n",
    "AttnX_bool = [1 if 'attnL' in x else 0 for x in AttnX_Condition]\n",
    "AttnX_bool = np.array(AttnX_bool)\n",
    "print(f'Total AttnL files: #{np.sum(AttnX_bool)}')\n",
    "# GoodSubjArr = np.ones(NumSubs)\n",
    "# BadSubs = ['2699']\n",
    "# # BadSubs = ['2663','2734','2652']\n",
    "# GoodSubs_bool = [1 if good_subj and x not in BadSubs else 0 for x, good_subj in zip(SubjID, GoodSubjArr)]\n",
    "# print(np.unique(GoodSubs_bool, return_counts = True))\n",
    "print(AttnX_bool[5:12])\n",
    "print(AttnX_Condition[5:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all averaged imaginary and real values across all good participants\n",
    "#### avg_complex_vals[8][2]\n",
    "#### 8 = 2f1 - 8f2\n",
    "##### 2 = PRE OR POST DATA\n",
    "#### [2x6x128] - 0 = real average number per channel, 1  = imag average number per ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# avg_complex_vals = {} # \n",
    "# for hemiharm in range(8):\n",
    "#     avg_complex_vals[hemiharm] = {}\n",
    "#     for pp in range(2):\n",
    "#         avg_complex_vals[hemiharm][pp] = AvgComplexVals(RealImaginary,xfxiter = int(hemiharm),ppi = pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# print(avg_complex_vals[0][0].shape)\n",
    "\n",
    "# data = avg_complex_vals[0][0]\n",
    "# fig,axs = plt.subplots(figsize = (10,3))\n",
    "\n",
    "\n",
    "# print(data[0,0,:].shape)\n",
    "# chlist = np.array(np.arange(0,NumChans,1))\n",
    "# print(chlist.shape)\n",
    "# for b in range(NumBins):\n",
    "#     axs.scatter(np.arange(0,NumChans,1),data[0,b,:])\n",
    "#     axs.vlines(SignificantChans, ymin = -1, ymax = 1, color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# pre_ri = avg_complex_vals[0][0]\n",
    "# post_ri = avg_complex_vals[0][1]\n",
    "\n",
    "# ppl = ['real','imaginary']\n",
    "\n",
    "# fig,axs = plt.subplots(6,2,figsize = (12,18), sharey = True)\n",
    "# for c in range(NumBins):\n",
    "#     for pp in range(2):\n",
    "#         axs[c,0].scatter(np.arange(0,NumChans,1),pre_ri[pp,c,:], color = rbow[pp], label = f'pre {ppl[pp]}')\n",
    "#         #axs[c,0].plot(pre_ri[pp,c,:],color = rbow[pp])\n",
    "#         axs[c,1].scatter(np.arange(0,NumChans,1),post_ri[pp,c,:], color = rbow[pp+3], label = f'post {ppl[pp]}')\n",
    "#         #axs[c,1].plot(post_ri[pp,c,:],color = rbow[pp+3])\n",
    "\n",
    "#         axs[c,0].legend(fontsize = 5)\n",
    "#         axs[c,1].legend(fontsize = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modify function and capture real / imagainary data per eahc subject for each real / imagary / pre/post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# def GetComplexValsPerSubj(RealImaginary,NumSubs,xfi,contrast_val):\n",
    "#     ''''\n",
    "#     This function iterates for:\n",
    "#     ONE XFX DATA KEY AND GETS ALL THE PRE POST COMPLEX VALUES FOR ALL \n",
    "#     PARTICIPANTS FOR ONE CONTRAST FOR EACH CHANNEL.\n",
    "#     RETURNS 2 keys [PRE/POST]: each with 3D ARRAY : [NumSubs x Real/Imaginary x Channels]\n",
    "\n",
    "#     xfi = 0-8 any 2F1 - 8F2 data \n",
    "#     contrast_val = Any value 0-5 [increasing contrast]\n",
    "#     '''\n",
    "#     prepost_complexs = {}\n",
    "\n",
    "#     for prepost in range(2):\n",
    "#         prepost_complexs[prepost] = {} # init pre == 0 and post  == 1 subkeys\n",
    "#         getcomplexs = np.zeros((NumSubs,2,NumChans)) # get each subjects complexs for each channel [2x128]\n",
    "#         for sIn in range(NumSubs):\n",
    "#             #complexIn = RealImaginary[sIn][xfxiter][prepost,contrast_val,:,:] # [real/imaginary vals / channels]\n",
    "#             getcomplexs[sIn,:]= RealImaginary[sIn][xfi][prepost,contrast_val,:,:]\n",
    "#         prepost_complexs[prepost] = getcomplexs\n",
    "\n",
    "#     return prepost_complexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# DataOut = {}\n",
    "# DataOut['RIVals'] = SubjectRealIMaginaryActivityPerChannel\n",
    "# # DataOut['SubjNames'] = \n",
    "# # DataOut['GoodSubjBool'] = \n",
    "# # DataOut['AttnXiInd'] = \n",
    "\n",
    "# SaveDataDir = 'C:\\\\plimon\\\\LTP_analysis\\\\ComplexValues_Data'\n",
    "# print(os.path.exists(SaveDataDir)) # does pth exist :p\n",
    "\n",
    "# dnt = datetime.now() # add date and time bc im wreckless when saving ..\n",
    "# fdnt = dnt.strftime(\"%Y%m%d_%H%M%S\") # set the above as a string ...\n",
    "# FileOutName = 'AllSubj_RealImagData_C1'\n",
    "# FileN = f'{FileOutName}_{fdnt}.pkl' \n",
    "# NewFileNPath = os.path.join(SaveDataDir,FileN)\n",
    "# print('Full New File Dir: ', NewFileNPath)\n",
    "\n",
    "# saveFile = 'y'\n",
    "# if saveFile == 'y':\n",
    "#  with open(NewFileNPath, 'wb') as file:\n",
    "#     pkl.dump(DataOut, file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "#     print('MAT Data For All Subjs Saved! :))')\n",
    "# else:\n",
    "#     print('Did Not Save File! Change file name before switching to y!')\n",
    "# print(os.path.exists(SaveDataDir)) # does pth exist :p\n",
    "\n",
    "\n",
    "# save = 'no'\n",
    "\n",
    "# if save == 'y':\n",
    "\n",
    "#     SaveDataDir = 'C:\\\\plimon\\\\LTP_analysis\\\\ComplexValues_Data'\n",
    "#     print(os.path.exists(SaveDataDir)) # does pth exist :p\n",
    "\n",
    "#     CoStrings = ['2F1','4F1','6F1','8F1','2F2','4F2','6F2','8F2']\n",
    "\n",
    "#     dnt = datetime.now() # add date and time bc im wreckless when saving ..\n",
    "#     fdnt = dnt.strftime(\"%Y%m%d_%H%M%S\") # set the above as a string ...\n",
    "\n",
    "#     for i in range(8):\n",
    "#         FileOutName = f'AllSubj_RealImagDataCo_{CoStrings[i]}'\n",
    "#         dataExport = np.array(SubjectRealIMaginaryActivityPerChannel[i])\n",
    "#         MatFileN = f'{FileOutName}_{fdnt}.mat' \n",
    "#         NewMatFileNPath = os.path.join(SaveDataDir,MatFileN)\n",
    "#         print('Full New File Dir: ', NewFileNPath)\n",
    "#         hdf5_file = h5py.File(NewMatFileNPath, 'w')\n",
    "#         hdf5_file.create_dataset('data', data=dataExport)\n",
    "#         hdf5_file.close()\n",
    "#         print(f'Data saved as {NewMatFileNPath}')\n",
    "# else:\n",
    "#     print(f'No Data Saved, switch to Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t2FC Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "allComplexVals = {}\n",
    "\n",
    "for acv in range(8):\n",
    "    allComplexVals[acv] = {}\n",
    "\n",
    "    SignificantChansperSweepInc = {}\n",
    "    # this loop performs pre post F1 and F2 test per sweep increment \n",
    "    for iters in range(NumBins):\n",
    "        sig_Chans = [] # store signifcant channels per contrast \n",
    "        # import 2F1 data for one contrast - computes real and imaginary data per post per key\n",
    "        pp_test = GetComplexValsPerSubj(NumSubs,NumChans,RealImaginary,xfi = 0,contrast_val = iters)\n",
    "\n",
    "        allComplexVals[acv] = pp_test\n",
    "\n",
    "        ### convert pre and post values into a [N x real/imaginary x pre/post]\n",
    "        for chan in range(NumChans):\n",
    "            prebin1DataCh = np.array(pp_test[0][:,:,chan])\n",
    "            postbin1DataCh = np.array(pp_test[1][:,:,chan])\n",
    "\n",
    "            res,b = t2FC(prebin1DataCh, postbin1DataCh, testMu=[0, 0], alphaVal=0.01)\n",
    "            #print(res)\n",
    "\n",
    "            if b == 1:\n",
    "                #print(f'{res}, Channels with significant diffs: {chan}')\n",
    "                sig_Chans.append(chan)\n",
    "                print(res)\n",
    "                # fig,axs = plt.subplots(figsize = (3,3),sharey = True)\n",
    "                # axs.scatter(prebin1DataCh,postbin1DataCh)\n",
    "                # axs.set_title(f'chan # {chan}')\n",
    "            SignificantChansperSweepInc[iters]  = sig_Chans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allComplexVals.keys())\n",
    "print(allComplexVals[0].keys())\n",
    "print(allComplexVals[5][1].shape)\n",
    "print(allComplexVals[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataOut = {}\n",
    "DataOut['avgComplexVals'] = allComplexVals\n",
    "\n",
    "SaveDataDir = 'C:\\\\plimon\\\\LTP_analysis\\\\eegMatFiles\\\\AllMat'\n",
    "print(os.path.exists(SaveDataDir)) # does pth exist :p\n",
    "\n",
    "dnt = datetime.now() # add date and time bc im wreckless when saving ..\n",
    "fdnt = dnt.strftime(\"%Y%m%d_%H%M%S\") # set the above as a string ...\n",
    "FileOutName = 'ComplexVals_AllSubs'\n",
    "FileN = f'{FileOutName}_{fdnt}.pkl' \n",
    "NewFileNPath = os.path.join(SaveDataDir,FileN)\n",
    "print('Full New File Dir: ', NewFileNPath)\n",
    "\n",
    "saveFile = 'y'\n",
    "if saveFile == 'y':\n",
    " with open(NewFileNPath, 'wb') as file:\n",
    "    pkl.dump(DataOut, file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    print('MAT Data For All Subjs Saved! :))')\n",
    "else:\n",
    "    print('Did Not Save File! Change file name before switching to y!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import scipy.io as sio\n",
    "\n",
    "DataOut = {}\n",
    "DataOut['avgComplexVals'] = allComplexVals\n",
    "\n",
    "SaveDataDir = 'C:\\\\plimon\\\\LTP_analysis\\\\eegMatFiles\\\\AllMat'\n",
    "print(os.path.exists(SaveDataDir))  # Check if the path exists\n",
    "\n",
    "dnt = datetime.now()  # Add date and time\n",
    "fdnt = dnt.strftime(\"%Y%m%d_%H%M%S\")  # Format date and time as a string\n",
    "\n",
    "FileOutName = 'ComplexVals_AllSubs'\n",
    "FileN = f'{FileOutName}_{fdnt}.mat'  # Change the file extension to .mat\n",
    "NewFileNPath = os.path.join(SaveDataDir, FileN)\n",
    "print('Full New File Dir:', NewFileNPath)\n",
    "\n",
    "saveFile = 'y'\n",
    "if saveFile == 'y':\n",
    "    sio.savemat(NewFileNPath, DataOut)  # Save data as .mat file\n",
    "    print('MAT Data For All Subjs Saved! :))')\n",
    "else:\n",
    "    print('Did Not Save File! Change file name before switching to y!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(SignificantChansperSweepInc.keys())\n",
    "\n",
    "allSigChans = []\n",
    "for i in range(6):\n",
    "    print(f'Significant chans for Bin# {i+1} {SignificantChansperSweepInc[i]}')\n",
    "    chiter = (len(SignificantChansperSweepInc[i]))\n",
    "    #print(chiter)\n",
    "    for sc in range(chiter):\n",
    "        chIn = (SignificantChansperSweepInc[i][sc])\n",
    "        allSigChans.append(chIn)\n",
    "    #allSigChans.append(SignificantChansperSweepInc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# for sweepInd in range(NumBins):\n",
    "#     fig,axes = init_TopoTemplate(f'2F1 - 2F2 PrePost Significant Chans At {cll[sweepInd]}')\n",
    "#     IdentifiedSigChans = np.array(SignificantChansperSweepInc[sweepInd])\n",
    "#     print(IdentifiedSigChans)\n",
    "#     # chMask = np.zeros(NumChans, dtype = bool)\n",
    "#     # chMask[IdentifiedSigChans] = False\n",
    "\n",
    "#     empty_mat =np.array(np.ones((NumChans)))\n",
    "#     #empty_mat[chMask == 1] = 1\n",
    "\n",
    "#     im,_ = plot_topomap(empty_mat,info,axes = axes,show =False,cmap = 'Spectral_r', res = 20, contours =2,sphere=(0.0,0.0,0.0,0.09),outlines = 'head')\n",
    "\n",
    "#     divider = make_axes_locatable(axes)\n",
    "#     cax = divider.append_axes('right', size='4%', pad=0.5)\n",
    "#     cbar = plt.colorbar(im, cax=cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# allSigChans = np.array(allSigChans)\n",
    "# SigChannels, reps = np.unique(allSigChans,return_counts = True)\n",
    "# print(f'Channels with signficant pre post difference per hemifield: {SigChannels}')\n",
    "# RepChans = SigChannels[reps >= 2] \n",
    "# print(f'Total Channels with identified significance {len(RepChans)}')\n",
    "# fig,axes = init_TopoTemplate(f'Stable Select Channels')\n",
    "# IdentifiedSigChans = np.array(SignificantChansperSweepInc[sweepInd])\n",
    "# chMask = np.zeros(NumChans, dtype = bool)\n",
    "# chMask[RepChans] = True\n",
    "# empty_mat =np.array(np.ones((NumChans)))\n",
    "# #empty_mat[chMask == 1] = 1\n",
    "# im,_ = plot_topomap(empty_mat,info,mask = chMask,axes = axes,show =False,cmap = 'Spectral_r', res = 20)\n",
    "# divider = make_axes_locatable(axes)\n",
    "# cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "# cbar = plt.colorbar(im, cax=cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# empty matricies to fill with all subjects data\n",
    "pre_2f1Data = np.zeros((NumSubs,NumBins,NumChans))\n",
    "post_2f1Data = np.zeros_like((pre_2f1Data))\n",
    "\n",
    "pre_2f2Data = np.zeros_like((pre_2f1Data))\n",
    "post_2f2Data = np.zeros_like((pre_2f1Data))\n",
    "\n",
    "for s in range(NumSubs):\n",
    "    # import and segment 2f1 data in pre post matricies \n",
    "    pre_2f1Data[s,:,:] = xFxData[s][0][0,:,:]\n",
    "    post_2f1Data[s,:,:] = xFxData[s][0][1,:,:]\n",
    "    # import and segment 2f2 data in pre post matricies \n",
    "    pre_2f2Data[s,:,:] = xFxData[s][4][0,:,:]\n",
    "    post_2f2Data[s,:,:] = xFxData[s][4][1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preF1M = np.nanmean(pre_2f1Data,axis = 0) # [6 x 128] = [NumBins x NumChans]\n",
    "# postF1M = np.nanmean(post_2f1Data,axis = 0)\n",
    "# preF2M = np.nanmean(pre_2f2Data,axis = 0)\n",
    "# postF2M = np.nanmean(post_2f2Data,axis = 0)\n",
    "\n",
    "\n",
    "# preF1M = np.nanmean(pre_2f1Data[AttnX_bool == 1,:,:],axis = 0) # [6 x 128] = [NumBins x NumChans]\n",
    "# postF1M = np.nanmean(post_2f1Data[AttnX_bool == 1,:,:],axis = 0)\n",
    "# preF2M = np.nanmean(pre_2f2Data[AttnX_bool == 1,:,:],axis = 0)\n",
    "# postF2M = np.nanmean(post_2f2Data[AttnX_bool == 1,:,:],axis = 0)\n",
    "\n",
    "\n",
    "# preF1M = np.nanmean(pre_2f1Data[AttnX_bool == 0,:,:],axis = 0) # [6 x 128] = [NumBins x NumChans]\n",
    "# postF1M = np.nanmean(post_2f1Data[AttnX_bool == 0,:,:],axis = 0)\n",
    "# preF2M = np.nanmean(pre_2f2Data[AttnX_bool == 0,:,:],axis = 0)\n",
    "# postF2M = np.nanmean(post_2f2Data[AttnX_bool == 0,:,:],axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(NumBins):\n",
    "\n",
    "    fig,axes = init_TopoTemplate(f'{cll[b]} Contrast')\n",
    "\n",
    "    pos1 = preF1M[b,:]\n",
    "    pos2 = postF1M[b,:]\n",
    "    pos3 = preF2M[b,:]\n",
    "    pos4 = postF2M[b,:]\n",
    "\n",
    "    # findLims = np.concatenate((pos1,pos2,pos3,pos4))\n",
    "    # print(findLims.shape)\n",
    "\n",
    "    # minV = np.min(findLims)\n",
    "    # maxV = np.max(findLims)\n",
    "\n",
    "    # set = [minV,maxV]\n",
    "    # print(set)\n",
    "\n",
    "    setMin = 0\n",
    "    setMax = 1.75\n",
    "    set = [setMin,setMax]\n",
    "\n",
    "    im,_ = plot_topomap(pos1,info,vlim = set,axes = axes[0],show =False,cmap = 'Spectral_r', res = 30,extrapolate = 'local', outlines = 'head',sphere=(0.0,0.0,0.0,0.09))\n",
    "    # divider = make_axes_locatable(axes[0])\n",
    "    # cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    # cbar = plt.colorbar(im, cax=cax)\n",
    "\n",
    "    im,_ = plot_topomap(pos2,info,vlim = set,axes = axes[1],show =False,cmap = 'Spectral_r', res = 30,extrapolate = 'local', outlines = 'head',sphere=(0.0,0.0,0.0,0.09))\n",
    "    # divider = make_axes_locatable(axes[1])\n",
    "    # cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    # cbar = plt.colorbar(im, cax=cax)\n",
    "\n",
    "    im,_ = plot_topomap(pos3,info,vlim = set,axes = axes[2],show =False,cmap = 'Spectral_r', res = 30,extrapolate = 'local', outlines = 'head',sphere=(0.0,0.0,0.0,0.09))\n",
    "    # divider = make_axes_locatable(axes[2])\n",
    "    # cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    # cbar = plt.colorbar(im, cax=cax)\n",
    "\n",
    "    im,_ = plot_topomap(pos4,info,vlim = set,axes = axes[3],show =False,cmap = 'Spectral_r', res = 30,extrapolate = 'local', outlines = 'head',sphere=(0.0,0.0,0.0,0.09))\n",
    "    # divider = make_axes_locatable(axes[3])\n",
    "    # cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    # cbar = plt.colorbar(im, cax=cax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot topos\n",
    "def init_TopoTemplate_DiffFigs(title_In):\n",
    "    \"\"\"Prep topo plot fig and axes\"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4), gridspec_kw=dict(height_ratios=[4]))\n",
    "    #axes.axis('off')\n",
    "    #axes[1].axis('off')\n",
    "    #axes.gridspec_kw=dict(height_ratios=[3])\n",
    "    #axes[1].gridspec_kw=dict(height_ratios=[3])\n",
    "    #plt.tight_layout()\n",
    "    plt.suptitle(f'{title_In}', fontsize = 20)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(NumBins): # NumBins\n",
    "\n",
    "    fig,axes = init_TopoTemplate_DiffFigs(f'{cll[b]} Contrast')\n",
    "\n",
    "    pos1 = preF1M[b,:]\n",
    "    pos2 = postF1M[b,:]\n",
    "\n",
    "    pos3 = preF2M[b,:]\n",
    "    pos4 = postF2M[b,:]     \n",
    "\n",
    "\n",
    "    # F1_Diff =pos2 - pos1 \n",
    "    # F2_Diff = (pos4 - pos3) \n",
    "\n",
    "    # F1_Diff =((pos2 - pos1) / (pos1 + pos2))\n",
    "    # F2_Diff = ((pos4 - pos3) /(pos3 + pos4))\n",
    "    \n",
    "    F1_Diff =((pos2 - pos1) / pos1)\n",
    "    F2_Diff = ((pos4 - pos3) /(pos3))\n",
    "\n",
    "\n",
    "    # limVer = np.concatenate((F1_Diff,F2_Diff),axis = 0)\n",
    "    # maxLim = np.max(limVer)\n",
    "    # minLim = np.min(limVer)\n",
    "\n",
    "    maxLim =0.1739\n",
    "    minLim =-0.1237\n",
    "    ampLims = [minLim,maxLim]\n",
    "\n",
    "    #print(ampLims)\n",
    "    im,_ = plot_topomap(F1_Diff,info,axes = axes[0],vlim = [minLim,maxLim],show =False,cmap = 'Spectral_r', res = 30,extrapolate = 'local', outlines = 'head',sphere=(0.0,0.0,0.0,0.09))\n",
    "    divider = make_axes_locatable(axes[1])\n",
    "    cax = divider.append_axes('right', size='3%', pad=0.5)\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "\n",
    "    im,_ = plot_topomap(F2_Diff,info,axes = axes[1],vlim = [minLim,maxLim],show =False,cmap = 'Spectral_r', res = 30,extrapolate = 'local', outlines = 'head',sphere=(0.0,0.0,0.0,0.09))\n",
    "    # divider = make_axes_locatable(axes[1])\n",
    "    # cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    # cbar = plt.colorbar(im, cax=cax)\n",
    "\n",
    "# -0.0659278929479683, 0.08002854526783483]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# test = GetComplexValsPerSubj(NumSubs,NumChans,RealImaginary,xfi = 0,contrast_val = 0)\n",
    "# t = test[0]\n",
    "# print(t.shape)\n",
    "\n",
    "# x_ch = t[:,:,78]\n",
    "# print(x_ch.shape)\n",
    "\n",
    "# x_hy = np.hypot(x_ch[:,0],x_ch[:,1])\n",
    "# print(x_hy.shape)\n",
    "\n",
    "# plt.scatter(np.arange(0,NumSubs,1), x_ch[:,0], label = 'reals', color = 'black')\n",
    "# plt.scatter(np.arange(0,NumSubs,1), x_ch[:,1], label = 'imaginaries', color = 'blue')\n",
    "\n",
    "# plt.scatter(np.arange(0,NumSubs,1), x_hy, label = 'hypotenuse', color = 'red')\n",
    "\n",
    "# plt.show()\n",
    "# plt.scatter(x_ch[:,0], x_ch[:,1], label = 'reals', color = 'black')\n",
    "# plt.hlines(0,xmin = -1,xmax = 1)\n",
    "# plt.vlines(0,ymin = -1, ymax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "xFxData[0][0].shape # 2 6 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toy data to make for hypothesis plots :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data = xFxData[11][0]\n",
    "\n",
    "#print(data.shape)\n",
    "\n",
    "\n",
    "pres = data[0,:,:]\n",
    "print(pres.shape)\n",
    "posts = data[1,:,:]\n",
    "\n",
    "\n",
    "\n",
    "crf_pre = np.nanmean(pres[:,40:95],axis = 1) # toy data to use \n",
    "crf_post = crf_pre + 0.5\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize = (9,4), sharey = True)\n",
    "axs[0].plot(crf_pre, label = 'Pre Induction', color = 'black')\n",
    "axs[0].plot(crf_post, label ='Post Induction', color = 'green')\n",
    "axs[0].set_ylabel(f'Amplitude (mV)')\n",
    "axs[0].set_xlabel(f' Contrast')\n",
    "axs[0].hlines(0, xmin = 0, xmax = 5,color = 'black')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(crf_pre, label = 'Pre Induction', color = 'brown', linewidth = 5,alpha = 0.5)\n",
    "axs[1].plot(crf_pre, label ='Post Induction', color = 'green',linewidth = 2,alpha = 0.5)\n",
    "axs[1].set_ylabel(f'Amplitude (mV)')\n",
    "axs[1].set_xlabel(f' Contrast')\n",
    "axs[1].hlines(0, xmin = 0, xmax = 5,color = 'black')\n",
    "axs[1].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
