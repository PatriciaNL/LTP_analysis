{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP ONE** OF DATA PROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This script imports data in which a Reliable Component Analysis was performed. Visual Stimulus was dual frequency tagged: F1 = 3 Hz(6), F2 = 3.75 Hs (7.5) (inverting stim). Each participant completed 2 sessions. For each session, a F1 and F2 bandpass filter was performed. So 1 single participant has 4 data files. This is for a frequency-based analysis of the LTP paradigm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np \n",
    "import scipy.io\n",
    "from scipy.io   import  loadmat\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt #import matplotlib as plt\n",
    "from scipy.optimize import curve_fit \n",
    "import seaborn as sns #import mat73\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Dir Path(s): MainDir, SaveDataDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files on hand: ['rcaResults_Sweep_contrast_sweeps_F1.mat', 'rcaResults_Sweep_contrast_sweeps_F2.mat']\n",
      "Current WD: C:\\plimon\\LTP_analysis\\RCA_F1\\RCA\\rcaResults_Sweep_contrast_sweeps_F1.mat\n",
      "Does File #1 Exist? True\n",
      "Current WD: C:\\plimon\\LTP_analysis\\RCA_F1\\RCA\\rcaResults_Sweep_contrast_sweeps_F2.mat\n",
      "Does File #2 Exist? True\n"
     ]
    }
   ],
   "source": [
    "# Main Directory of processed file from MatLab\n",
    "#MainDir = 'D:\\\\AttnXV3_analysis\\\\RCA_F1\\\\RCA\\\\' # set dir - with USB Drive\n",
    "MainDir = 'C:\\\\plimon\\\\LTP_analysis\\\\RCA_F1\\\\RCA\\\\' # set dir - on my computer\n",
    "os.chdir(MainDir) # change old dir, to this dir\n",
    "d = os.listdir(MainDir) # list files in dir\n",
    "print(f'Files on hand: {d}')\n",
    "##############################################\n",
    "FileN_f1 = d[0] # choose one\n",
    "FileN_f2  = d[1]                         \n",
    "file_path1 = os.path.join(MainDir, FileN_f1) # join paths and prep 2 load\n",
    "print('Current WD:',file_path1) # does path exist ... ?\n",
    "print('Does File #1 Exist?',os.path.exists(file_path1)) # yes or no\n",
    "\n",
    "file_path2 = os.path.join(MainDir, FileN_f2) # join paths and prep 2 load\n",
    "print('Current WD:',file_path2) # does path exist ... ?\n",
    "print('Does File #2 Exist?',os.path.exists(file_path1)) # yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to save data:C:\\plimon\\LTP_analysis\\RCA_F1\\AllSubjSweepRCA\\\n",
      "Full New File Dir:  C:\\plimon\\LTP_analysis\\RCA_F1\\AllSubjSweepRCA\\AllRCAData_pnlApp_20240229_110438.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save Data Dir ...\n",
    "#SaveDataDir = 'D:\\\\AttnXV3_analysis\\\\RCA_F1\\\\AllSubjSweepRCA\\\\' # set dir where files (.pkl, .csv) will be saved\n",
    "SaveDataDir = 'C:\\\\plimon\\\\LTP_analysis\\\\RCA_F1\\\\AllSubjSweepRCA\\\\' # set dir where files (.pkl, .csv) will be saved\n",
    "FileOutName = 'AllRCAData_pnlApp' \n",
    "newPath = os.path.join(SaveDataDir, FileOutName)\n",
    "if not os.path.exists(SaveDataDir):\n",
    "    os.makedirs(SaveDataDir)\n",
    "print(f'Path to save data:{SaveDataDir}')\n",
    "######################################################\n",
    "dnt = datetime.now() # add date and time bc im wreckless when saving ..\n",
    "fdnt = dnt.strftime(\"%Y%m%d_%H%M%S\") # set the above as a string ...\n",
    "FileN = f'{FileOutName}_{fdnt}.pkl' \n",
    "#FileNToMatlab = f'{FileOutName}_{fdnt}.h5' \n",
    "#FileN = f'{FileOutName}_{fdnt}.csv' \n",
    "NewFileNPath = os.path.join(SaveDataDir,FileN)\n",
    "print('Full New File Dir: ', NewFileNPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1 = scipy.io.loadmat(file_path1)\n",
    "df_f2 = scipy.io.loadmat(file_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load F1 and F2 RCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rca_f1 = df_f1['rcaResult']['projectedData'][0,0]\n",
    "f1 = [rca_f1[x,0] for x in range(rca_f1.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rca_f2 = df_f2['rcaResult']['projectedData'][0,0]\n",
    "f2 = [rca_f2[x,0] for x in range(rca_f2.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Files: 55\n"
     ]
    }
   ],
   "source": [
    "# load subject names ...\n",
    "SubNames = df_f1['rcaResult'][0,0][5]\n",
    "FileName = [x[0][3:] for subjlist in SubNames for x in subjlist[0][2][0]]\n",
    "#FileName = np.sort(FileName)\n",
    "print(f'Total Data Files: {len(FileName)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Set some params we'll need\n",
    "[NumCols, NumComps, b] = np.shape(f1[0]) # 24 x 4 x n-Trials\n",
    "NumFiles = int(len(FileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find How Many Subject Names There are ...\n",
    "string_ind = '_'\n",
    "uniqueSubs = []\n",
    "SessFileType = []\n",
    "for n in range(NumFiles):\n",
    "    x = FileName[n] # single file name string ie: 'nl-xxxx_attnX'\n",
    "    if string_ind in x:\n",
    "        y = x.split(string_ind)[0] \n",
    "        z = x.split(string_ind)[1]\n",
    "        # from each session name extract import info that will help us index and exclude later on ..\n",
    "        uniqueSubs.append(y) # store all participant numbers (repeating - will be sorted later)\n",
    "        SessFileType.append(z) # store the condtion label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Participants: 29\n",
      "26 Participants completed the study\n",
      "3 Participants did not complete the study\n"
     ]
    }
   ],
   "source": [
    "# given double sessions.., how many unique participants do we have?\n",
    "TotalFiles = np.array(uniqueSubs)\n",
    "[NumSubs, SessCounts] = np.unique(TotalFiles, return_counts = True) # returns unique subject and how many sessions they did (should be 2)\n",
    "print(f'Total Participants: {len(NumSubs)}')\n",
    "[FileQuants, TotSess] = np.unique(SessCounts, return_counts = True) # returns counts of how many subs did 1 session and 2 sessions \n",
    "print(f'{TotSess[1]} Participants completed the study')\n",
    "print(f'{TotSess[0]} Participants did not complete the study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects with 2 files: 29\n",
      "['2651' '2652' '2653' '2654' '2655' '2657' '2658' '2659' '2661' '2663'\n",
      " '2664' '2665' '2666' '2667' '2668' '2669' '2670' '2671' '2672' '2674'\n",
      " '2677' '2695' '2696' '2697' '345202' '345216']\n",
      "\n",
      "Single Subjs: ['2660' '2676' '2678']\n"
     ]
    }
   ],
   "source": [
    "TwoSess = SessCounts == 2 # index of who completed 2 sessions ..\n",
    "GoodSubjs = NumSubs[TwoSess] # subs who completed 2 sessions \n",
    "SingleSessSubjs = NumSubs[~TwoSess] # subs who did not complete 2 sessions \n",
    "print(f'Subjects with 2 files: {len(NumSubs)}')\n",
    "print(GoodSubjs)\n",
    "print()\n",
    "print(f'Single Subjs: {SingleSessSubjs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have to add this step because I ran someone for attnR **twice** ... yikes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2663 did not match file name for attnL, moving subj to proper file ind array\n"
     ]
    }
   ],
   "source": [
    "# makes sure paticipants did complete attnL and attnR\n",
    "FNameCrit = ['attnL', 'attnR'] # session names \n",
    "CleanSubjs = [] \n",
    "\n",
    "for i in range(len(GoodSubjs)):\n",
    "    subj = GoodSubjs[i]\n",
    "    f_list = [x for x in FileName if subj in x] # import all strongs were sub number is \n",
    "    list_check  = np.sort(f_list) # abc order strings -  attnL and THEN attnR\n",
    "    counter = 0\n",
    "    for n in range(len(FNameCrit)):\n",
    "        if FNameCrit[n] in list_check[n]: # expt label should match file name in same position\n",
    "            counter = counter + 1 # if so add 1 \n",
    "            if (n == 1) and (counter == 2): # if both files strings are different, append\n",
    "                CleanSubjs.append(GoodSubjs[i])\n",
    "        else: # if not, add em to the singletons ... \n",
    "            print(f'{GoodSubjs[i]} did not match file name for {FNameCrit[n]}, moving subj to proper file ind array')\n",
    "            str_nm = (np.array([GoodSubjs[i]], dtype=object))\n",
    "            SingleSubs = np.concatenate((str_nm, SingleSessSubjs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many participants **actually** completed both sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Participants Completed AttnL and AttnR\n",
      "4 Participants did not properly complete all sessions\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(CleanSubjs)} Participants Completed AttnL and AttnR')\n",
    "print(f'{len(SingleSubs)} Participants did not properly complete all sessions')\n",
    "#print(CleanSubjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Data Files as AttnL and AttnR in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNameCrit = ['attnL', 'attnR']\n",
    "FilePos = np.zeros((len(CleanSubjs), 2))\n",
    "\n",
    "for name in range(len(CleanSubjs)):\n",
    "    yIn = CleanSubjs[name] # import single subject who completed 2 sessions \n",
    "    all_files_avil = [x for x in FileName if yIn in x] # list\n",
    "    all_files_avil = np.sort(all_files_avil) # might not be necessary but jic ...\n",
    "    pos = [loc for loc, file in enumerate(FileName) if file in all_files_avil] # index position of files if files match attnL and attnR\n",
    "    FilePos[name,:] = pos\n",
    "#print(FilePos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a big for loop to save all this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttnL =  {'F1': [], 'F2': []} \n",
    "AttnR =  {'F1': [], 'F2': []} \n",
    "#AttnX =  {'F1': [], 'F2': []} \n",
    "\n",
    "# run a different file we imported (2 F1 and F2 filtered data) to save all in the same file\n",
    "for iter in range(NumFiles):\n",
    "    for ind in range(len(CleanSubjs)):\n",
    "        attnL_FilePos = int(FilePos[ind,0])\n",
    "        attnR_FilePos = int(FilePos[ind,1])\n",
    "        if iter == 0:\n",
    "            data = f1\n",
    "            AttnL['F1'].append(data[attnL_FilePos])  # Append value to list in 'F1' key\n",
    "            AttnR['F1'].append(data[attnR_FilePos])  # Append value to list in 'F1' key\n",
    "        elif iter == 1:\n",
    "            data = f2\n",
    "            AttnL['F2'].append(data[attnL_FilePos])  # Append value to list in 'F2' key\n",
    "            AttnR['F2'].append(data[attnR_FilePos])  # Append value to list in 'F2' key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Saving Subjects data who only completed 1 session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2663_attnR2_20231030_1659' '2663_attnR_20231019_1018'\n",
      " '2660_attnL_20231017_1102' '2676_attnR_20231027_1002'\n",
      " '2678_attnR_20231101_1446']\n"
     ]
    }
   ],
   "source": [
    "SingleFiles_arr = []\n",
    "for file in range(len(SingleSubs)):\n",
    "    subIn = SingleSubs[file] # import one file at a time\n",
    "    files_avil = [x for x in FileName if subIn in x]\n",
    "    SingleFiles_arr.append(files_avil)\n",
    "\n",
    "SinglefileNames = np.array(list(chain(*SingleFiles_arr))) #all single session names flattened\n",
    "print(SinglefileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_catch_L = 'attnL'\n",
    "string_catch_R = 'attnR'\n",
    "single_sess_ind = np.zeros(len(SinglefileNames)) # size of single sessions available\n",
    "SingleSessSubName = [] # save subject name \n",
    "single_sess_pos = [] # find and store the data index \n",
    "\n",
    "for sInFName in range(len(SinglefileNames)):\n",
    "    # determin whether its attnr or attnL\n",
    "    fIn = SinglefileNames[sInFName]\n",
    "    #find the postion of file in the data to organize later\n",
    "    pos = [posi for posi, file in enumerate(FileName) if file in fIn]\n",
    "    single_sess_pos.append(pos)\n",
    "\n",
    "    x = fIn.split(string_ind)[1]\n",
    "    y = fIn.split(string_ind)[0]\n",
    "    SingleSessSubName.append(y)\n",
    "    # make array to findex what files are attnL and attnR\n",
    "    if string_catch_L in x:\n",
    "        single_sess_ind[sInFName] = 1 # attnL ind == 1\n",
    "    elif string_catch_R in x:\n",
    "        single_sess_ind[sInFName] = 0 # attnL ind == 0\n",
    "\n",
    "single_sess_pos = np.array(single_sess_pos)\n",
    "# print(single_sess_pos)\n",
    "# print(single_sess_ind)\n",
    "# print(SingleSessSubName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index and save singles sessions as a seperate dict to export in pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sess_AttnL =  {'F1': [], 'F2': []} \n",
    "single_sess_AttnR =  {'F1': [], 'F2': []} \n",
    "l_subs = []\n",
    "r_subs = []\n",
    "\n",
    "for file_op in range(NumFiles):\n",
    "    for oneSess in range(len(SinglefileNames)):\n",
    "        sIn = SingleSessSubName[oneSess] # single sub names\n",
    "        AttnXCond = single_sess_ind[oneSess] # condtion they did\n",
    "        DataPos = single_sess_pos[oneSess] # position of data file is \n",
    "        DataPos = int(DataPos[0]) \n",
    "\n",
    "        if file_op == 0:\n",
    "            dataIn = f1 # switch files 2 combine them\n",
    "            if AttnXCond == 1:\n",
    "                single_sess_AttnL['F1'].append(dataIn[DataPos]) # save data in this dict\n",
    "                l_subs.append(sIn) # save subject name in this dict\n",
    "            else:\n",
    "                single_sess_AttnR['F1'].append(dataIn[DataPos])\n",
    "                r_subs.append(sIn)\n",
    "\n",
    "        elif file_op == 1:\n",
    "            dataIn = f2 # switch files 2 combine them\n",
    "            if AttnXCond == 1:\n",
    "                single_sess_AttnL['F2'].append(dataIn[DataPos])\n",
    "            else:\n",
    "                single_sess_AttnR['F2'].append(dataIn[DataPos])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 'AttnLSubNames', 'AttnRSubNames'])\n"
     ]
    }
   ],
   "source": [
    "SingleSessDataOut = {}\n",
    "\n",
    "SingleSessDataOut[0] = single_sess_AttnL['F1']\n",
    "SingleSessDataOut[1] = single_sess_AttnL['F2']\n",
    "SingleSessDataOut[2] = single_sess_AttnR['F1']\n",
    "SingleSessDataOut[3] = single_sess_AttnR['F2']\n",
    "\n",
    "SingleSessDataOut['AttnLSubNames'] = np.array(l_subs)\n",
    "SingleSessDataOut['AttnRSubNames'] = np.array(r_subs)\n",
    "#SingleSessDataOut['DataNotes'] = ['keys: 0&1 attnL[f1/f2] and 2&3 attR[f1/f2], single session data']\n",
    "print(SingleSessDataOut.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 'FullSessSubjNames', 'DataNotes'])\n"
     ]
    }
   ],
   "source": [
    "SessDataOut = {}\n",
    "\n",
    "SessDataOut[0] = AttnL['F1']\n",
    "SessDataOut[1] = AttnL['F2']\n",
    "SessDataOut[2] = AttnR['F1']\n",
    "SessDataOut[3] = AttnR['F2']\n",
    "\n",
    "SessDataOut['FullSessSubjNames'] = CleanSubjs\n",
    "SessDataOut['DataNotes'] = ['keys: 0&1 attnL[f1/f2] and 2&3 attR[f1/f2]']\n",
    "print(SessDataOut.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOut = {}\n",
    "\n",
    "dataOut[0] = SessDataOut\n",
    "dataOut[1] = SingleSessDataOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data into .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataOut = dict()\n",
    "\n",
    "# dataOut['FullSessSubjNames'] = CleanSubjs\n",
    "# #dataOut['OneSessSubjNames'] = CleanSubjs\n",
    "# dataOut['DataNotes'] = ['keys: 0&1 attnL[f1/f2] and 2&3 attR[f1/f2]']\n",
    "# dataOut[0] = AttnL['F1']\n",
    "# dataOut[1] = AttnL['F2']\n",
    "\n",
    "# dataOut[2] = AttnR['F1']\n",
    "# dataOut[3] = AttnR['F2']\n",
    "\n",
    "# print(dataOut.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Data Saved! :))\n"
     ]
    }
   ],
   "source": [
    "saveFile = 'y'\n",
    "\n",
    "if saveFile == 'y':\n",
    " with open(NewFileNPath, 'wb') as file:\n",
    "    pkl.dump(dataOut, file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    print('Sorted Data Saved! :))')\n",
    "else:\n",
    "    print('Did Not Save File! Change file name before switching to y!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
