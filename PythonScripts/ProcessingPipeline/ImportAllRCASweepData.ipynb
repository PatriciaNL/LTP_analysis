{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP ONE** OF DATA PROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This script imports data in which a Reliable Component Analysis was performed. Visual Stimulus was dual frequency tagged: F1 = 3 Hz(6), F2 = 3.75 Hs (7.5) (inverting stim). Each participant completed 2 sessions. For each session, a F1 and F2 bandpass filter was performed. So 1 single participant has 4 data files. This is for a frequency-based analysis of the LTP paradigm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np \n",
    "import scipy.io\n",
    "from scipy.io   import  loadmat\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt #import matplotlib as plt\n",
    "from scipy.optimize import curve_fit \n",
    "import seaborn as sns #import mat73\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Dir Path(s): MainDir, SaveDataDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files on hand: ['rcaResults_Sweep_contrast sweeps_F1_031324.mat', 'rcaResults_Sweep_contrast sweeps_F2_031324.mat', 'rcaResults_Sweep_contrast_sweeps_F1.mat', 'rcaResults_Sweep_contrast_sweeps_F2.mat']\n",
      "Current WD: C:\\plimon\\LTP_analysis\\RCA_F1\\RCA\\rcaResults_Sweep_contrast sweeps_F1_031324.mat\n",
      "Does File #1 Exist? True\n",
      "Current WD: C:\\plimon\\LTP_analysis\\RCA_F1\\RCA\\rcaResults_Sweep_contrast sweeps_F2_031324.mat\n",
      "Does File #2 Exist? True\n"
     ]
    }
   ],
   "source": [
    "# Main Directory of processed file from MatLab\n",
    "#MainDir = 'D:\\\\AttnXV3_analysis\\\\RCA_F1\\\\RCA\\\\' # set dir - with USB Drive\n",
    "MainDir = 'C:\\\\plimon\\\\LTP_analysis\\\\RCA_F1\\\\RCA\\\\' # set dir - on my computer\n",
    "os.chdir(MainDir) # change old dir, to this dir\n",
    "d = os.listdir(MainDir) # list files in dir\n",
    "print(f'Files on hand: {d}')\n",
    "##############################################\n",
    "FileN_f1 = d[0] # choose one\n",
    "FileN_f2  = d[1]                         \n",
    "file_path1 = os.path.join(MainDir, FileN_f1) # join paths and prep 2 load\n",
    "print('Current WD:',file_path1) # does path exist ... ?\n",
    "print('Does File #1 Exist?',os.path.exists(file_path1)) # yes or no\n",
    "\n",
    "file_path2 = os.path.join(MainDir, FileN_f2) # join paths and prep 2 load\n",
    "print('Current WD:',file_path2) # does path exist ... ?\n",
    "print('Does File #2 Exist?',os.path.exists(file_path1)) # yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to save data:C:\\plimon\\LTP_analysis\\RCA_F1\\AllSubjSweepRCA\\\n",
      "Full New File Dir:  C:\\plimon\\LTP_analysis\\RCA_F1\\AllSubjSweepRCA\\AllRCAData_pnlApp_20240313_105950.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save Data Dir ...\n",
    "#SaveDataDir = 'D:\\\\AttnXV3_analysis\\\\RCA_F1\\\\AllSubjSweepRCA\\\\' # set dir where files (.pkl, .csv) will be saved\n",
    "SaveDataDir = 'C:\\\\plimon\\\\LTP_analysis\\\\RCA_F1\\\\AllSubjSweepRCA\\\\' # set dir where files (.pkl, .csv) will be saved\n",
    "FileOutName = 'AllRCAData_pnlApp' \n",
    "newPath = os.path.join(SaveDataDir, FileOutName)\n",
    "if not os.path.exists(SaveDataDir):\n",
    "    os.makedirs(SaveDataDir)\n",
    "print(f'Path to save data:{SaveDataDir}')\n",
    "######################################################\n",
    "dnt = datetime.now() # add date and time bc im wreckless when saving ..\n",
    "fdnt = dnt.strftime(\"%Y%m%d_%H%M%S\") # set the above as a string ...\n",
    "FileN = f'{FileOutName}_{fdnt}.pkl' \n",
    "#FileNToMatlab = f'{FileOutName}_{fdnt}.h5' \n",
    "#FileN = f'{FileOutName}_{fdnt}.csv' \n",
    "NewFileNPath = os.path.join(SaveDataDir,FileN)\n",
    "print('Full New File Dir: ', NewFileNPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1 = scipy.io.loadmat(file_path1)\n",
    "df_f2 = scipy.io.loadmat(file_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load F1 and F2 RCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rca_f1 = df_f1['rcaResult']['projectedData'][0,0]\n",
    "f1 = [rca_f1[x,0] for x in range(rca_f1.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rca_f2 = df_f2['rcaResult']['projectedData'][0,0]\n",
    "f2 = [rca_f2[x,0] for x in range(rca_f2.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Files: 66\n"
     ]
    }
   ],
   "source": [
    "# load subject names ...\n",
    "SubNames = df_f1['rcaResult'][0,0][5]\n",
    "FileName = [x[0][3:] for subjlist in SubNames for x in subjlist[0][2][0]]\n",
    "#FileName = np.sort(FileName)\n",
    "print(f'Total Data Files: {len(FileName)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2651-attnL-20231003-1500', '2651-attnR-20231006-0933', '2652-attnL-20231003-1635', '2652-attnR-20231011-1328', '2653-attnL-20231009-1015', '2653-attnR-20231013-0932', '2654-attnL-20231009-1131', '2654-attnR-20231016-1152', '2655-attnL-20231009-1303', '2655-attnR-20231016-0948', '2657-attnL-20231013-1508', '2657-attnR-20231020-1201', '2658-attnL-20231013-1639', '2658-attnR-20231020-1052', '2659-attnL-20231017-0940', '2659-attnR-20231018-1523', '2660-attnL-20231017-1102', '2661-attnL-20231018-1322', '2661-attnR-20231017-1358', '2663-attnR2-20231030-1659', '2663-attnR-20231019-1018', '2664-attnL-20231020-1452', '2664-attnR-20231019-1145', '2665-attnL-20231024-1329', '2665-attnR-20231019-1512', '2666-attnL-20231023-1612', '2666-attnR-20231019-1643', '2667-attnL-20231025-1522', '2667-attnR-20231023-0947', '2668-attnL-20231027-1251', '2668-attnR-20231023-1058', '2669-attnL-20231024-1432', '2669-attnR-20231023-1241', '2670-attnL-20231201-1521', '2670-attnR-20231024-0941', '2671-attnL-20231024-1107', '2671-attnR-20231025-1419', '2672-attnL-20231030-1540', '2672-attnR-20231025-1305', '2674-attnL-20231026-1107', '2674-attnR-20231031-1054', '2676-attnR-20231027-1002', '2677-attnL-20231101-1552', '2677-attnR-20231030-1204', '2678-attnR-20231101-1446', '2695-attnL-20231122-1414', '2695-attnR-20231120-1204', '2696-attnL-20231201-1157', '2696-attnR-20231120-1430', '2697-attnL-20231120-1601', '2697-attnR-20231122-1251', '2708-attnL-20240119-1029', '2715-attnL-20240129-1138', '2716-attnL-20240129-1253', '2726-attnR-20240221-1133', '2727-attnL-20240221-1316', '2728-attnL-20240305-1534', '2728-attnR-20240221-1431', '2733-attnL-20240312-1701', '2734-attnL-20240312-0954', '345202-attnL-20230929-1605', '345202-attnR-20231027-1621', '345215-attnL-20240130-1514', '345215-attnR-20240201-1637', '345216-attnL-20230929-1732', '345216-attnR-20231027-1449']\n"
     ]
    }
   ],
   "source": [
    "# set all file names to the same hyphen format\n",
    "FileName = [FileName.replace('_','-') for FileName in FileName]\n",
    "print(FileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Set some params we'll need\n",
    "[NumCols, NumComps, b] = np.shape(f1[0]) # 24 x 4 x n-Trials\n",
    "NumFiles = int(len(FileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find How Many Subject Names There are ...\n",
    "string_ind = '-'\n",
    "uniqueSubs = []\n",
    "SessFileType = []\n",
    "for n in range(NumFiles):\n",
    "    x = FileName[n] # single file name string ie: 'nl-xxxx_attnX'\n",
    "    if string_ind in x:\n",
    "        y = x.split(string_ind)[0] # subj number\n",
    "        z = x.split(string_ind)[1] # session condition name \n",
    "        # from each session name extract import info that will help us index and exclude later on ..\n",
    "        uniqueSubs.append(y) # store all participant numbers (repeating - will be sorted later)\n",
    "        SessFileType.append(z) # store the condtion label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Participants: 38\n",
      "28 Participants completed the study\n",
      "10 Participants did not complete the study\n"
     ]
    }
   ],
   "source": [
    "# given double sessions.., how many unique participants do we have?\n",
    "TotalFiles = np.array(uniqueSubs)\n",
    "[NumSubs, SessCounts] = np.unique(TotalFiles, return_counts = True) # returns unique subject and how many sessions they did (should be 2)\n",
    "print(f'Total Participants: {len(NumSubs)}')\n",
    "[FileQuants, TotSess] = np.unique(SessCounts, return_counts = True) # returns counts of how many subs did 1 session and 2 sessions \n",
    "print(f'{TotSess[1]} Participants completed the study')\n",
    "print(f'{TotSess[0]} Participants did not complete the study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects with 2 files: 38\n",
      "['2651' '2652' '2653' '2654' '2655' '2657' '2658' '2659' '2661' '2663'\n",
      " '2664' '2665' '2666' '2667' '2668' '2669' '2670' '2671' '2672' '2674'\n",
      " '2677' '2695' '2696' '2697' '2728' '345202' '345215' '345216']\n",
      "\n",
      "Single Subjs: ['2660' '2676' '2678' '2708' '2715' '2716' '2726' '2727' '2733' '2734']\n"
     ]
    }
   ],
   "source": [
    "TwoSess = SessCounts == 2 # index of who completed 2 sessions ..\n",
    "GoodSubjs = NumSubs[TwoSess] # subs who completed 2 sessions \n",
    "SingleSessSubjs = NumSubs[~TwoSess] # subs who did not complete 2 sessions \n",
    "print(f'Subjects with 2 files: {len(NumSubs)}')\n",
    "print(GoodSubjs)\n",
    "print()\n",
    "print(f'Single Subjs: {SingleSessSubjs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have to add this step because I ran someone for attnR **twice** ... yikes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2663 did not match file name for attnL, moving subj to proper file ind array\n"
     ]
    }
   ],
   "source": [
    "# makes sure paticipants did complete attnL and attnR\n",
    "FNameCrit = ['attnL', 'attnR'] # session names \n",
    "CleanSubjs = [] \n",
    "\n",
    "for i in range(len(GoodSubjs)):\n",
    "    subj = GoodSubjs[i]\n",
    "    f_list = [x for x in FileName if subj in x] # import all strongs were sub number is \n",
    "    list_check  = np.sort(f_list) # abc order strings -  attnL and THEN attnR\n",
    "    counter = 0\n",
    "    for n in range(len(FNameCrit)):\n",
    "        if FNameCrit[n] in list_check[n]: # expt label should match file name in same position\n",
    "            counter = counter + 1 # if so add 1 \n",
    "            if (n == 1) and (counter == 2): # if both files strings are different, append\n",
    "                CleanSubjs.append(GoodSubjs[i])\n",
    "        else: # if not, add em to the singletons ... \n",
    "            print(f'{GoodSubjs[i]} did not match file name for {FNameCrit[n]}, moving subj to proper file ind array')\n",
    "            str_nm = (np.array([GoodSubjs[i]], dtype=object))\n",
    "            SingleSubs = np.concatenate((str_nm, SingleSessSubjs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many participants **actually** completed both sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Participants Completed AttnL and AttnR\n",
      "11 Participants did not properly complete all sessions\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(CleanSubjs)} Participants Completed AttnL and AttnR')\n",
    "print(f'{len(SingleSubs)} Participants did not properly complete all sessions')\n",
    "#print(CleanSubjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Data Files as AttnL and AttnR in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNameCrit = ['attnL', 'attnR']\n",
    "FilePos = np.zeros((len(CleanSubjs), 2))\n",
    "\n",
    "for name in range(len(CleanSubjs)):\n",
    "    yIn = CleanSubjs[name] # import single subject who completed 2 sessions \n",
    "    all_files_avil = [x for x in FileName if yIn in x] # list\n",
    "    all_files_avil = np.sort(all_files_avil) # might not be necessary but jic ...\n",
    "    pos = [loc for loc, file in enumerate(FileName) if file in all_files_avil] # index position of files if files match attnL and attnR\n",
    "    FilePos[name,:] = pos\n",
    "#print(FilePos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a big for loop to save all this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttnL =  {'F1': [], 'F2': []} \n",
    "AttnR =  {'F1': [], 'F2': []} \n",
    "#AttnX =  {'F1': [], 'F2': []} \n",
    "\n",
    "# run a different file we imported (2 F1 and F2 filtered data) to save all in the same file\n",
    "for iter in range(NumFiles):\n",
    "    for ind in range(len(CleanSubjs)):\n",
    "        attnL_FilePos = int(FilePos[ind,0])\n",
    "        attnR_FilePos = int(FilePos[ind,1])\n",
    "        if iter == 0:\n",
    "            data = f1\n",
    "            AttnL['F1'].append(data[attnL_FilePos])  # Append value to list in 'F1' key\n",
    "            AttnR['F1'].append(data[attnR_FilePos])  # Append value to list in 'F1' key\n",
    "        elif iter == 1:\n",
    "            data = f2\n",
    "            AttnL['F2'].append(data[attnL_FilePos])  # Append value to list in 'F2' key\n",
    "            AttnR['F2'].append(data[attnR_FilePos])  # Append value to list in 'F2' key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Saving Subjects data who only completed 1 session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2663-attnR2-20231030-1659' '2663-attnR-20231019-1018'\n",
      " '2660-attnL-20231017-1102' '2676-attnR-20231027-1002'\n",
      " '2678-attnR-20231101-1446' '2708-attnL-20240119-1029'\n",
      " '2715-attnL-20240129-1138' '2716-attnL-20240129-1253'\n",
      " '2726-attnR-20240221-1133' '2727-attnL-20240221-1316'\n",
      " '2733-attnL-20240312-1701' '2734-attnL-20240312-0954']\n"
     ]
    }
   ],
   "source": [
    "SingleFiles_arr = []\n",
    "for file in range(len(SingleSubs)):\n",
    "    subIn = SingleSubs[file] # import one file at a time\n",
    "    files_avil = [x for x in FileName if subIn in x]\n",
    "    SingleFiles_arr.append(files_avil)\n",
    "\n",
    "SinglefileNames = np.array(list(chain(*SingleFiles_arr))) #all single session names flattened\n",
    "print(SinglefileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_catch_L = 'attnL'\n",
    "string_catch_R = 'attnR'\n",
    "single_sess_ind = np.zeros(len(SinglefileNames)) # size of single sessions available\n",
    "SingleSessSubName = [] # save subject name \n",
    "single_sess_pos = [] # find and store the data index \n",
    "\n",
    "for sInFName in range(len(SinglefileNames)):\n",
    "    # determin whether its attnr or attnL\n",
    "    fIn = SinglefileNames[sInFName]\n",
    "    #find the postion of file in the data to organize later\n",
    "    pos = [posi for posi, file in enumerate(FileName) if file in fIn]\n",
    "    single_sess_pos.append(pos)\n",
    "\n",
    "    x = fIn.split(string_ind)[1]\n",
    "    y = fIn.split(string_ind)[0]\n",
    "    SingleSessSubName.append(y)\n",
    "    # make array to findex what files are attnL and attnR\n",
    "    if string_catch_L in x:\n",
    "        single_sess_ind[sInFName] = 1 # attnL ind == 1\n",
    "    elif string_catch_R in x:\n",
    "        single_sess_ind[sInFName] = 0 # attnL ind == 0\n",
    "\n",
    "single_sess_pos = np.array(single_sess_pos)\n",
    "# print(single_sess_pos)\n",
    "# print(single_sess_ind)\n",
    "# print(SingleSessSubName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index and save singles sessions as a seperate dict to export in pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sess_AttnL =  {'F1': [], 'F2': []} \n",
    "single_sess_AttnR =  {'F1': [], 'F2': []} \n",
    "l_subs = []\n",
    "r_subs = []\n",
    "\n",
    "for file_op in range(NumFiles):\n",
    "    for oneSess in range(len(SinglefileNames)):\n",
    "        sIn = SingleSessSubName[oneSess] # single sub names\n",
    "        AttnXCond = single_sess_ind[oneSess] # condtion they did\n",
    "        DataPos = single_sess_pos[oneSess] # position of data file is \n",
    "        DataPos = int(DataPos[0]) \n",
    "\n",
    "        if file_op == 0:\n",
    "            dataIn = f1 # switch files 2 combine them\n",
    "            if AttnXCond == 1:\n",
    "                single_sess_AttnL['F1'].append(dataIn[DataPos]) # save data in this dict\n",
    "                l_subs.append(sIn) # save subject name in this dict\n",
    "            else:\n",
    "                single_sess_AttnR['F1'].append(dataIn[DataPos])\n",
    "                r_subs.append(sIn)\n",
    "\n",
    "        elif file_op == 1:\n",
    "            dataIn = f2 # switch files 2 combine them\n",
    "            if AttnXCond == 1:\n",
    "                single_sess_AttnL['F2'].append(dataIn[DataPos])\n",
    "            else:\n",
    "                single_sess_AttnR['F2'].append(dataIn[DataPos])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 'AttnLSubNames', 'AttnRSubNames'])\n"
     ]
    }
   ],
   "source": [
    "SingleSessDataOut = {}\n",
    "\n",
    "SingleSessDataOut[0] = single_sess_AttnL['F1']\n",
    "SingleSessDataOut[1] = single_sess_AttnL['F2']\n",
    "SingleSessDataOut[2] = single_sess_AttnR['F1']\n",
    "SingleSessDataOut[3] = single_sess_AttnR['F2']\n",
    "\n",
    "SingleSessDataOut['AttnLSubNames'] = np.array(l_subs)\n",
    "SingleSessDataOut['AttnRSubNames'] = np.array(r_subs)\n",
    "#SingleSessDataOut['DataNotes'] = ['keys: 0&1 attnL[f1/f2] and 2&3 attR[f1/f2], single session data']\n",
    "print(SingleSessDataOut.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 'FullSessSubjNames', 'DataNotes'])\n"
     ]
    }
   ],
   "source": [
    "SessDataOut = {}\n",
    "\n",
    "SessDataOut[0] = AttnL['F1']\n",
    "SessDataOut[1] = AttnL['F2']\n",
    "SessDataOut[2] = AttnR['F1']\n",
    "SessDataOut[3] = AttnR['F2']\n",
    "\n",
    "SessDataOut['FullSessSubjNames'] = CleanSubjs\n",
    "SessDataOut['DataNotes'] = ['keys: 0&1 attnL[f1/f2] and 2&3 attR[f1/f2]']\n",
    "print(SessDataOut.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data into .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOut = {}\n",
    "\n",
    "dataOut[0] = SessDataOut\n",
    "dataOut[1] = SingleSessDataOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataOut = dict()\n",
    "\n",
    "# dataOut['FullSessSubjNames'] = CleanSubjs\n",
    "# #dataOut['OneSessSubjNames'] = CleanSubjs\n",
    "# dataOut['DataNotes'] = ['keys: 0&1 attnL[f1/f2] and 2&3 attR[f1/f2]']\n",
    "# dataOut[0] = AttnL['F1']\n",
    "# dataOut[1] = AttnL['F2']\n",
    "\n",
    "# dataOut[2] = AttnR['F1']\n",
    "# dataOut[3] = AttnR['F2']\n",
    "\n",
    "# print(dataOut.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Data Saved! :))\n"
     ]
    }
   ],
   "source": [
    "saveFile = 'y'\n",
    "\n",
    "if saveFile == 'y':\n",
    " with open(NewFileNPath, 'wb') as file:\n",
    "    pkl.dump(dataOut, file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    print('Sorted Data Saved! :))')\n",
    "else:\n",
    "    print('Did Not Save File! Change file name before switching to y!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
