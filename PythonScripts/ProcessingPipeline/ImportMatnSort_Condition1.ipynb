{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libs\n",
    "import numpy as np \n",
    "import scipy \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt #import matplotlib as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns #import mat73\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "import mne\n",
    "from mne.viz import plot_topomap\n",
    "from mne.viz import set_3d_title, set_3d_view\n",
    "sns.set_theme() # set the plotting atmosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important functions\n",
    "def GetDims(DictIn):\n",
    "    \"\"\"This script gets the dims of data \n",
    "    for later indexing\"\"\"\n",
    "    iter = int(len(DictIn.keys())-1)\n",
    "    NumDims = np.zeros((int(iter),3))\n",
    "    for Sind in range(iter):\n",
    "        NumDims[Sind,:] = DictIn[Sind].shape\n",
    "    return NumDims\n",
    "    \n",
    "def DataSort(dictIn, sIn):\n",
    "    \"\"\"This function takes 1 subjects data.\n",
    "        Data contains 6 keys per bin.\n",
    "        Returns: Pre, Post and Post-Pre, and 2*(Post-Pre)/(Pre+Post) \"\"\"\n",
    "    BinDiffArith_Out = {}\n",
    "    BinBaseDiff_Out = {}\n",
    "    BinPrePost_Out = {}\n",
    "    fx = dictIn[sIn] # import data \n",
    "\n",
    "    for harmonic in range(NumHarms):\n",
    "\n",
    "        fx_avg = np.zeros((NumBins,int(len(chanList)),2)) # bins x channel x pre/post\n",
    "        fx_diff = np.zeros((NumBins,int(len(chanList))))\n",
    "        fx_sum = np.zeros_like(fx_diff)\n",
    "        fx_diffArith = np.zeros_like(fx_diff)\n",
    "\n",
    "        nt = int((fx[harmonic].shape[2])/2)\n",
    "        fx_temp = np.reshape(fx[harmonic], newshape = (NumBins,int(NumChans),2,nt))\n",
    "        #get pre and post induction average\n",
    "        for induCo in range(2):\n",
    "            fx_avg[:,:,induCo] = np.nanmean(fx_temp[:,:,induCo,:], axis = 2) # 6 x 128 \n",
    "        for bin in range(NumBins):\n",
    "            fx_diff[bin,:] = fx_avg[bin,:,1] - fx_avg[bin,:,0] # post - pre\n",
    "            fx_sum[bin,:] = fx_avg[bin,:,1] + fx_avg[bin,:,0] \n",
    "            fx_diffArith[bin,:] = 2*(((fx_diff[bin,:])/(fx_sum[bin,:])))\n",
    "\n",
    "        BinDiffArith_Out[harmonic] = fx_diffArith\n",
    "        BinBaseDiff_Out[harmonic] = fx_diff\n",
    "        BinPrePost_Out[harmonic] = fx_avg\n",
    "\n",
    "    return BinDiffArith_Out,BinBaseDiff_Out,BinPrePost_Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all files that will be sorted and exported from + into another .mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Directory of processed file from MatLab\n",
    "#MainDir = 'D:\\\\AttnXV3_analysis\\\\RCA_F1\\\\AvgCRFs\\\\' # set dir\n",
    "MainDir = 'C:\\\\plimon\\\\LTP_analysis\\\\eegMatFiles\\\\AllMAT' # set dir\n",
    "os.chdir(MainDir) # change old dir, to this dir\n",
    "dataFileNames = os.listdir(MainDir) # list files in dir\n",
    "print(f'Total Files Avilable: {len(dataFileNames)}')\n",
    "##############################################\n",
    "FileN = dataFileNames[0]# choose one                        \n",
    "file_path1 = os.path.join(MainDir, FileN) # join paths and prep 2 load\n",
    "print('Current WD:',file_path1) # does path exist ... ?\n",
    "print('Does File #1 Exist?',os.path.exists(file_path1)) # yes or no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Participant FileName and Expt Session Condition: AttnL == 1 or AttnR == 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumBins = 6\n",
    "NumHarms = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Keys in original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadData = pkl.load(open(file_path1, 'rb'))\n",
    "print(loadData.keys())\n",
    "print(len(loadData['RawData'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumSubs = int(len(loadData['SubjNames']))\n",
    "print(f'Subject Files Found: {NumSubs}')\n",
    "files = loadData['SubjNames']\n",
    "\n",
    "file_label = [files.replace('_','-') for files in files] # set files in single format \n",
    "print((file_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_ind = '-'\n",
    "\n",
    "SubjName = [] # subject data in \n",
    "ExptSession = [] # AttnL, AttnR\n",
    "\n",
    "for s in range(NumSubs):\n",
    "    fnameIn = file_label[s]\n",
    "    sNum = fnameIn.split(string_ind)[1] # subject name \n",
    "    exptSess = fnameIn.split(string_ind)[2] # expt session performed\n",
    "\n",
    "    SubjName.append(sNum)\n",
    "    ExptSession.append(exptSess)\n",
    "\n",
    "    ExptSess_bool = [1 if x == 'attnL' else 0 for x in ExptSession] # attnL == 1 , attnR == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### small sanity check for file labels and correct sorting ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = 11\n",
    "print(file_label[si], SubjName[si], ExptSession[si],ExptSess_bool[si])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExptInformation = {}\n",
    "\n",
    "ExptInformation['OrignalFileName'] = file_label\n",
    "ExptInformation['FileSubjNumber'] = SubjName\n",
    "ExptInformation['attnX_label'] = ExptSession\n",
    "ExptInformation['attnX_binarybool'] = ExptSess_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dims = GetDims(loadData['RawData']) # shape of all subjs data\n",
    "\n",
    "print(f'Variability of Rows: {np.unique(Dims[:,0],return_counts = True)}')\n",
    "print(f'Variability of Chans: {np.unique(Dims[:,1])}')\n",
    "print(f'Variability of Trials: {np.unique(Dims[:,2], return_counts = True)}')\n",
    "NumChans = ((np.unique(Dims[:,1])))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_iters = int(Dims.shape[0])\n",
    "\n",
    "splitRowInd = []\n",
    "splitCondInd = []\n",
    "\n",
    "for sIn in range(s_iters):\n",
    "    dataDimsIn= Dims[sIn,:] # info for indexing rows, cols and trials\n",
    "    splitRowInd.append(int((dataDimsIn[0]/2))) # cutoff for splitting data along real-imag axis\n",
    "    splitCondInd.append(int(dataDimsIn[2]/2)) # cutoff for splitting trials as pre and post  \n",
    "splitRowInd = np.array(splitRowInd)\n",
    "splitCondInd = np.array(splitCondInd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Channels of Interest to Visualize Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elecLeftLateral=[51,52,60,58,59,64,65,68,69]\n",
    "elecLeftMedial=[72,75,81,70,71,74]\n",
    "elecRightMedial=[72,75,81,76,83,82]\n",
    "elecMedial=[72,75,81,70,71,74,76,83,82]\n",
    "elecRightLateral=[97,92,85,96,91,95,90,94,88]\n",
    "extraChans = np.arange(50,100,1)\n",
    "extra = [107,101,47,42,37,31,102,46,41,45,49,103,108,113,108,99]\n",
    "AnalysisChans = [elecLeftLateral,elecLeftMedial,elecRightMedial,elecMedial,elecRightLateral,extraChans,extra]\n",
    "ExportChans = np.unique(np.sort(np.concatenate(AnalysisChans))) # Combine array for chans of interest\n",
    "VisChans  = ExportChans - 1\n",
    "print(f'Channels to be visualized: {VisChans}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Bool mask for MNE Topoplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chanList = np.arange(0,Dims[0,1],1)\n",
    "ocm = np.zeros_like((chanList))\n",
    "ocm[VisChans] = True # set good chans to true\n",
    "\n",
    "ocm_chans = ocm.astype(bool)\n",
    "print(ocm_chans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Real and Imaginary Components of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe has to add a mask before hypot is computed\n",
    "cleanData = {}\n",
    "for sIn in range(s_iters):\n",
    "    RawData = loadData['RawData'][sIn]\n",
    "     # reshape data so 0 = real and 1 = imaginary values \n",
    "    reshapeData  = RawData.reshape((splitRowInd[sIn]),2,int(Dims[sIn,1]),int(Dims[sIn,2]))\n",
    "    # replace bad chans with nan's\n",
    "    reshapeData[reshapeData == 0] = np.nan\n",
    "    # empty dataframe\n",
    "    comboData = np.zeros((splitRowInd[sIn],int(Dims[sIn,1]),int(Dims[sIn,2])))\n",
    "    unique_Iter = splitRowInd[sIn]\n",
    "    for val in range(unique_Iter):\n",
    "        comboData[val,:,:] = np.hypot(reshapeData[val,0,:,:],reshapeData[val,1,:,:])\n",
    "    cleanData[sIn] = comboData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data Per Bin and Store in Dict, Remove Prelude Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preBins = NumBins + 1 #(+1 bc prelude bin)\n",
    "GoodBinData = {}\n",
    "\n",
    "for sIn in range(s_iters):\n",
    "    dIn = cleanData[sIn] # import 1 subjs data \n",
    "    rmvBinInd = np.array(np.arange(0,splitRowInd[sIn],preBins)) # set indices to remove 1st (oth bin) out of every 7th\n",
    "    ind_arr = np.ones(splitRowInd[sIn]) # all rows good as default\n",
    "    ind_arr[rmvBinInd] = False # id prelude bins\n",
    "    GoodBinData[sIn] = dIn[ind_arr == 1,:,:] # save into dict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Point #1** : Make sure 1F1 harmonic data is **NOT** included in analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HarmonicDataCo = 4 # should be 2f 4 f 6f and 8f data\n",
    "HemiFieldDataCo = 2 # should exist for f1 and f2\n",
    "idealHarmOrd = HarmonicDataCo*HemiFieldDataCo # 8\n",
    "\n",
    "allHemiData = {}\n",
    "for sIn in range(s_iters):\n",
    "# if data along rows is not divisible, remove 1f1 data\n",
    "    if (GoodBinData[sIn].shape[0] % idealHarmOrd) != 0:\n",
    "        print(f'Data File # {sIn} contains 1F1 data, now removing ...')\n",
    "        allHemiData[sIn] = GoodBinData[sIn][NumBins:,:,:] # remove the 1st 6 values of data \n",
    "    else:\n",
    "        print(f'File # {sIn} was preserved')\n",
    "        allHemiData[sIn] = GoodBinData[sIn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 and F2 data are stacked on top of each other so here we split them :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xF1HemiData = {} # data is : 2f1 4f1 6f1 8f1\n",
    "xF2HemiData = {} # data is  2f2 4f2 6f2 8f2\n",
    "\n",
    "for sIn in range(s_iters):\n",
    "    HarmonicCutoff = int(allHemiData[sIn].shape[0] / 2)\n",
    "    xF1HemiData[sIn] = np.array(allHemiData[sIn][:HarmonicCutoff,:,:]) # harmonics x channels x trials  = 4 x 128 x 78\n",
    "    xF2HemiData[sIn] = np.array(allHemiData[sIn][HarmonicCutoff:,:,:]) # harmonics x channels x trials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xF1HemiBins = {}\n",
    "xF2HemiBins = {}\n",
    "for sIn in range(s_iters):\n",
    "    x1 = xF1HemiData[sIn]\n",
    "    x2 = xF2HemiData[sIn]\n",
    "    HarmonicEpochs = np.array(np.arange(0,x1.shape[0],NumBins)) # index to segment bins\n",
    "\n",
    "    xF1HemiBins[sIn] = {}\n",
    "    xF2HemiBins[sIn] = {}\n",
    "    for xFX in range(HarmonicDataCo):\n",
    "        xF1HemiBins[sIn][xFX] = x1[HarmonicEpochs[xFX]:HarmonicEpochs[xFX]+(NumBins),:,:]\n",
    "        xF2HemiBins[sIn][xFX] = x2[HarmonicEpochs[xFX]:HarmonicEpochs[xFX]+(NumBins),:,:]\n",
    "        #print(xF1HemiBins[sIn][xFX].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seperate data into pre and post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bda_f1 = {}\n",
    "bbd_f1 = {}\n",
    "bpp_f1 = {}\n",
    "for s in range(s_iters):\n",
    "    #print(f'now sorting data file # {s}')\n",
    "    bda_f1[s], bbd_f1[s], bpp_f1[s] = DataSort(xF1HemiBins, sIn = s)\n",
    "\n",
    "F1Out = {}\n",
    "F1Out['BinDiffArith'] =  bda_f1 # 2*(post-pre)/post+pre\n",
    "F1Out['BinBaseDiff'] = bbd_f1   # post - pre\n",
    "F1Out['BinPrePost'] =  bpp_f1   # pre AND post data is in these subkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bda_f2 = {}\n",
    "bbd_f2 = {}\n",
    "bpp_f2 = {}\n",
    "for s in range(s_iters):\n",
    "    #print(f'now sorting data file # {s}')\n",
    "    bda_f2[s], bbd_f2[s], bpp_f2[s] = DataSort(xF2HemiBins, sIn = s)\n",
    "\n",
    "F2Out = {}\n",
    "F2Out['BinDiffArith'] =  bda_f2 # 2*(post-pre)/post+pre\n",
    "F2Out['BinBaseDiff'] = bbd_f2   # post - pre\n",
    "F2Out['BinPrePost'] =  bpp_f2   # pre AND post data is in these subkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataOut = {}\n",
    "DataOut['F1_Data'] = F1Out # all F1 data\n",
    "DataOut['F2_Data'] = F2Out # all F2 data\n",
    "DataOut['eegSubInfo'] = ExptInformation # arrays needed for indexing what groups to average together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataOutDir = 'C:\\\\plimon\\\\LTP_analysis\\\\eegClean'\n",
    "FileOutName = 'ProcessedData' \n",
    "dnt = datetime.now() # add date and time bc im wreckless when saving ..\n",
    "fdnt = dnt.strftime(\"%Y%m%d_%H%M%S\") # set the above as a string ...\n",
    "FileN = f'{FileOutName}_{fdnt}.pkl'  # make proper file out name\n",
    "\n",
    "NewFileNPath = os.path.join(DataOutDir,FileN) # path where data will be \n",
    "print('Full New File Dir: ', NewFileNPath) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile = 'y'\n",
    "\n",
    "if saveFile == 'y':\n",
    " with open(NewFileNPath, 'wb') as file:\n",
    "    pkl.dump(DataOut, file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    print('Sorted Data Saved! :))')\n",
    "else:\n",
    "    print('Did Not Save File! Change file name before switching to y!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select EEG Montage for MNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montageIn = mne.channels.make_standard_montage(\"GSN-HydroCel-128\")\n",
    "info = mne.create_info(ch_names=montageIn.ch_names, sfreq=1, ch_types=\"eeg\")\n",
    "info.set_montage(montageIn)\n",
    "print(montageIn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NumBins):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8, 10),gridspec_kw=dict(height_ratios=[3]))\n",
    "    axes.set_title(f' Bin # {(i+1)}', fontsize = 30)\n",
    "    axes.axis('off')\n",
    "\n",
    "    dataIn = np.array(bda_f2[10][0][i,:]) # plot data per bin\n",
    "    print(dataIn.shape)\n",
    "    # hashtag below to plot all channel activity\n",
    "    dataIn[~ocm_chans] = 0 # want to index good channels only and set the rest channels to 0 value\n",
    "    im,_ = plot_topomap(dataIn, info,mask = ocm, axes = axes, show =False,cmap = 'Spectral_r', res = 32, contours = 1)\n",
    "    divider = make_axes_locatable(axes)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topodata = p[0,:,:].T\n",
    "# print(topodata.shape)\n",
    "# emptyChData = np.zeros((1,6))\n",
    "# print(emptyChData.shape)\n",
    "\n",
    "# dataFull = np.array(np.concatenate((topodata,emptyChData)))\n",
    "\n",
    "# data = dataFull[:,4]\n",
    "\n",
    "# # create array with 4 points for our 4 channels\n",
    "# # in the same order as provided in ch_names\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 10),gridspec_kw=dict(height_ratios=[3]))\n",
    "# axes.set_title(f'subj topo plot', fontsize = 30)\n",
    "# # placeholder = np.random.random((10, 10))\n",
    "# # axes.imshow(placeholder)\n",
    "# axes.axis('off')\n",
    "\n",
    "# #info = create_info(ch_names=montageIn._names, sfreq=1, ch_types='eeg')\n",
    "# # channel names I provided are part of a standard montage\n",
    "# #info.set_montage(montageIn) # import file)\n",
    "# im,_ = plot_topomap(data, info, axes = axes, show =False,cmap = 'Spectral_r', res = 32, contours = 8)\n",
    "# divider = make_axes_locatable(axes)\n",
    "# cax = divider.append_axes('right', size='5%', pad=0.2)\n",
    "# cbar = plt.colorbar(im, cax=cax)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good confined example of what data formate is supposed to look like for mne plotting success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mne import create_info\n",
    "# from mne.viz import plot_topomap\n",
    "# import numpy as np\n",
    "\n",
    "# # create array with 4 points for our 4 channels\n",
    "# # in the same order as provided in ch_names\n",
    "# data = np.random.randn(4)  \n",
    "# info = create_info(ch_names=['CPz', 'Oz', 'POz', 'Fz'], sfreq=1000, ch_types='eeg')\n",
    "# # channel names I provided are part of a standard montage\n",
    "# info.set_montage('standard_1020')\n",
    "\n",
    "# plot_topomap(data, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topodata = p[0,:,:].T  # 128 x 6 - 0 -== 2f\n",
    "# #have to add some fck channel data because egi file included one extra channel\n",
    "# emptyChData = np.zeros((1,6)) # 1 x 6\n",
    "# data = np.array(np.concatenate((topodata,emptyChData))) # 129 x 6 \n",
    "\n",
    "\n",
    "# for i in range(6):\n",
    "#     #plt.plot(topodata[:,i])\n",
    "#     fig, axs = plt.subplots(figsize = (6,6))\n",
    "#     axs.set_title(f'Bin # {i}')\n",
    "#     axs.legend()\n",
    "#     print(i)\n",
    "#     dataIn = data[:,i]\n",
    "#     mne.viz.plot_topomap(dataIn, info, axes = axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot formats of the past and present(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (10,10))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# montageIn.plot(kind='3d', axes=ax)\n",
    "# ax.set_title('Sensor Locations')\n",
    "# plt.show()\n",
    "# # Plot the sensors on a 2D head model\n",
    "# fig, ax = plt.subplots()\n",
    "# montageIn.plot(kind='topomap', show_names=True, axes=ax)\n",
    "# ax.set_title('Sensor Locations (2D)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #montge default info in case egi outut file fails me \n",
    "# montage_type = mne.channels.make_standard_montage(\"GSN-HydroCel-128\")\n",
    "# print(montage_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_label = ['2F','4F','6F','8F']\n",
    "\n",
    "# fig,axs = plt.subplots(6,1, figsize = (10,40))\n",
    "# for i in range(6):\n",
    "#     dIn = axs[i].imshow(XF1_Data[0][:,i,ExportChans],aspect='auto', cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "#     axs[i].set_xticks(np.arange(len(ExportChans)))\n",
    "#     axs[i].set_xticklabels(ExportChans,rotation=45)\n",
    "#     axs[i].set_yticks(np.arange(len(h_label)))  # Set y-ticks to match the length of y_labels\n",
    "#     axs[i].set_yticklabels(h_label) \n",
    "#     axs[i].set_title('Pre F1 Data')\n",
    "#     axs[i].set_xlabel('Channels')\n",
    "#     axs[i].set_ylabel('Amplitude')\n",
    "#plt.show()\n",
    "# fig,axs = plt.subplots(6,1, figsize = (10,40))\n",
    "# for i in range(6):\n",
    "#     dIn = axs[i].imshow(XF1_Data[1][:,i,ExportChans],aspect='auto', cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "#     axs[i].set_xticks(np.arange(len(ExportChans)))\n",
    "#     axs[i].set_xticklabels(ExportChans,rotation=45)\n",
    "#     axs[i].set_yticks(np.arange(len(h_label)))  # Set y-ticks to match the length of y_labels\n",
    "#     axs[i].set_yticklabels(h_label) \n",
    "#     axs[i].set_title('Post F1 Data')\n",
    "#     axs[i].set_xlabel('Channels')\n",
    "#     axs[i].set_ylabel('Amplitude')\n",
    "# fig,axs = plt.subplots(6,1, figsize = (10,40))\n",
    "# for i in range(6):\n",
    "#     dIn = axs[i].imshow(XF2_Data[0][:,i,ExportChans],aspect='auto', cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "#     axs[i].set_xticks(np.arange(len(ExportChans)))\n",
    "#     axs[i].set_xticklabels(ExportChans,rotation=45)\n",
    "#     axs[i].set_yticks(np.arange(len(h_label)))  # Set y-ticks to match the length of y_labels\n",
    "#     axs[i].set_yticklabels(h_label) \n",
    "#     axs[i].set_title('Pre F2 Data')\n",
    "#     axs[i].set_xlabel('Channels')\n",
    "#     axs[i].set_ylabel('Amplitude')\n",
    "# fig,axs = plt.subplots(6,1, figsize = (10,40))\n",
    "# for i in range(6):\n",
    "#     dIn = axs[i].imshow(XF2_Data[1][:,i,ExportChans],aspect='auto', cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "#     axs[i].set_xticks(np.arange(len(ExportChans)))\n",
    "#     axs[i].set_xticklabels(ExportChans,rotation=45)\n",
    "#     axs[i].set_yticks(np.arange(len(h_label)))  # Set y-ticks to match the length of y_labels\n",
    "#     axs[i].set_yticklabels(h_label) \n",
    "#     axs[i].set_title('Post F2 Data')\n",
    "#     axs[i].set_xlabel('Channels')\n",
    "#     axs[i].set_ylabel('Amplitude')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
