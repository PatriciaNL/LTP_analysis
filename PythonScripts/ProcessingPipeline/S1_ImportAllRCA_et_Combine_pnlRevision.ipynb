{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP ONE** OF DATA PROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This script imports data in which a Reliable Component Analysis was performed. Visual Stimulus was dual frequency tagged: 2F1 = 6 Hz, 2F2 = 7.5 Hz (inverting stim). Each participant completed 2 sessions. For each session, a xF1 and xF2 bandpass filter was performed. So 1 single participant has 4 data files. This is for a frequency-based analysis of the LTP paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np \n",
    "import scipy.io\n",
    "from scipy.io   import  loadmat\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt #import matplotlib as plt\n",
    "from scipy.optimize import curve_fit \n",
    "import seaborn as sns #import mat73\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Dir Path(s): MainDir, SaveDataDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviable Files to choose from: 4\n",
      "Files on hand: ['OfficialControlsF1_rcaResults_Sweep_contrast sweeps.mat', 'OfficialControls_F2_rcaResults_Sweep_contrast sweeps.mat', 'OfficialF1_rcaResults_Sweep_F2F4F6F8.mat', 'OfficialF2_rcaResults_Sweep_F2F4F6F8.mat']\n",
      "Current WD: C:\\plimon\\LTP_analysis\\RCA_F1\\RCA\\OfficialF1_rcaResults_Sweep_F2F4F6F8.mat\n",
      "Does File #1 Exist? True\n",
      "Current WD: C:\\plimon\\LTP_analysis\\RCA_F1\\RCA\\OfficialF2_rcaResults_Sweep_F2F4F6F8.mat\n",
      "Does File #2 Exist? True\n"
     ]
    }
   ],
   "source": [
    "# Main Directory of processed file from MatLab\n",
    "MainDir = 'C:\\\\plimon\\\\LTP_analysis\\\\RCA_F1\\\\RCA\\\\' # set dir - on my computer\n",
    "os.chdir(MainDir) # change old dir, to this dir\n",
    "d = os.listdir(MainDir) # list files in dir\n",
    "print(f'Aviable Files to choose from: {len(d)}')\n",
    "print(f'Files on hand: {d}')\n",
    "##############################################\n",
    "FileN_f1 = d[2] # choose one\n",
    "FileN_f2  = d[3]                         \n",
    "file_path1 = os.path.join(MainDir, FileN_f1) # join paths and prep 2 load\n",
    "print('Current WD:',file_path1) # does path exist ... ?\n",
    "print('Does File #1 Exist?',os.path.exists(file_path1)) # yes or no\n",
    "\n",
    "file_path2 = os.path.join(MainDir, FileN_f2) # join paths and prep 2 load\n",
    "print('Current WD:',file_path2) # does path exist ... ?\n",
    "print('Does File #2 Exist?',os.path.exists(file_path1)) # yes or no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Data Dir ...\n",
    "# #SaveDataDir = 'D:\\\\AttnXV3_analysis\\\\RCA_F1\\\\AllSubjSweepRCA\\\\' # set dir where files (.pkl, .csv) will be saved\n",
    "# SaveDataDir = 'C:\\\\plimon\\\\LTP_analysis\\\\RCA_F1\\\\' # set dir where files (.pkl, .csv) will be saved\n",
    "# FileOutName = 'AllSessionDataCombin_allHarms' \n",
    "# newPath = os.path.join(SaveDataDir, FileOutName)\n",
    "# if not os.path.exists(SaveDataDir):\n",
    "#     os.makedirs(SaveDataDir)\n",
    "# print(f'Path to save data:{SaveDataDir}')\n",
    "# ######################################################\n",
    "# dnt = datetime.now() # add date and time bc im wreckless when saving ..\n",
    "# fdnt = dnt.strftime(\"%Y%m%d_%H%M%S\") # set the above as a string ...\n",
    "# FileN = f'{FileOutName}_{fdnt}.pkl' \n",
    "# #FileNToMatlab = f'{FileOutName}_{fdnt}.h5' \n",
    "# #FileN = f'{FileOutName}_{fdnt}.csv' \n",
    "# NewFileNPath = os.path.join(SaveDataDir,FileN)\n",
    "# print('Full New File Dir: ', NewFileNPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1 = scipy.io.loadmat(file_path1)\n",
    "df_f2 = scipy.io.loadmat(file_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load F1 and F2 RCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rca_f1 = df_f1['rcaResult']['projectedData'][0,0]\n",
    "f1 = [rca_f1[x,0] for x in range(rca_f1.shape[0])] # entry per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rca_f2 = df_f2['rcaResult']['projectedData'][0,0]\n",
    "f2 = [rca_f2[x,0] for x in range(rca_f2.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Files: 66\n"
     ]
    }
   ],
   "source": [
    "# load subject names ...\n",
    "SubNames = df_f1['rcaResult'][0,0][5]\n",
    "FileName = [x[0][3:] for subjlist in SubNames for x in subjlist[0][2][0]]\n",
    "#FileName = np.sort(FileName)\n",
    "print(f'Total Data Files: {len(FileName)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Subj Session Name Format Uniform and extract sub ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all file names to the same hyphen format\n",
    "FileName = [FileName.replace('_','-') for FileName in FileName]\n",
    "#print(len(FileName))\n",
    "#### Set some params we'll need\n",
    "#[NumCols, NumComps, b] = np.shape(f1[0]) # 24 x 4 x n-Trials\n",
    "NumFiles = int(len(FileName))\n",
    "\n",
    "#print(NumCols,NumCmps, b) # new data dims \n",
    "# Find How Many Subject Names There are ...\n",
    "string_ind = '-'\n",
    "uniqueSubs = []\n",
    "SessFileType = []\n",
    "'for each file name, get sub ID and also session record'\n",
    "for n in range(NumFiles):\n",
    "    x = FileName[n] # single file name string ie: 'nl-xxxx_attnX'\n",
    "    if string_ind in x:\n",
    "        y = x.split(string_ind)[0] # get subj number\n",
    "        #print(y)\n",
    "        z = x.split(string_ind)[1] # get session condition name \n",
    "        #print(z)\n",
    "        # from each session name extract import info that will help us index and exclude later on ..\n",
    "        uniqueSubs.append(y) # store all participant numbers (repeating - will be sorted later)\n",
    "        SessFileType.append(z) # store the condtion label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject IDs: ['2651', '2651', '2652', '2652', '2653', '2653', '2654', '2654', '2655', '2655', '2657', '2657', '2658', '2658', '2659', '2659', '2660', '2661', '2661', '2663', '2663', '2664', '2664', '2665', '2665', '2666', '2666', '2667', '2667', '2668', '2668', '2669', '2669', '2670', '2670', '2671', '2671', '2672', '2672', '2674', '2674', '2676', '2677', '2677', '2678', '2695', '2695', '2696', '2696', '2697', '2697', '2708', '2715', '2716', '2726', '2727', '2728', '2728', '2733', '2734', '345202', '345202', '345215', '345215', '345216', '345216']\n",
      " Conditions: ['attnL', 'attnR']\n"
     ]
    }
   ],
   "source": [
    "print(f'Subject IDs: {uniqueSubs}')\n",
    "print(f' Conditions: {SessFileType[0:2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Participants: 38\n"
     ]
    }
   ],
   "source": [
    "# given double sessions.., how many unique participants do we have?\n",
    "TotalFiles = np.array(uniqueSubs)\n",
    "[NumSubs, SessCounts] = np.unique(TotalFiles, return_counts = True) # returns unique subject and how many sessions they did (should be 2)\n",
    "print(f'Total Participants: {len(NumSubs)}')\n",
    "[FileQuants, TotSess] = np.unique(SessCounts, return_counts = True) # returns counts of how many subs did 1 session and 2 sessions \n",
    "# print(f'{TotSess[1]} Participants completed the study')\n",
    "# print(f'{TotSess[0]} Participants did not complete the study')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoSess = SessCounts == 2 # index of who completed 2 sessions ..\n",
    "GoodSubjs = NumSubs[TwoSess] # subs who completed 2 sessions \n",
    "SingleSessSubjs = NumSubs[~TwoSess] # subs who did not complete 2 sessions \n",
    "#print(f'Subjects with 2 files: {len(GoodSubjs)}')\n",
    "#print(GoodSubjs)\n",
    "#print()\n",
    "#print(f'Single Subjs: {SingleSessSubjs}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is a specific group of subjects to look at, here is where subj ID's can be grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet_Inds = ['2651','2658','2659','2661','2664','2666','2668','2672','2674','2695','345202','345215','2627','2715','2726']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have to add this step because I ran someone for attnR **twice** ... yikes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2663 did not match file name for attnL, moving subj to proper file ind array\n"
     ]
    }
   ],
   "source": [
    "# makes sure paticipants did complete attnL and attnR\n",
    "FNameCrit = ['attnL', 'attnR'] # session names \n",
    "CleanSubjs = []\n",
    " \n",
    "for i in range(len(GoodSubjs)):\n",
    "    subj = GoodSubjs[i]\n",
    "    f_list = [x for x in FileName if subj in x] # import all strongs were sub number is \n",
    "    list_check  = np.sort(f_list) # abc order strings -  attnL and THEN attnR\n",
    "    counter = 0\n",
    "    for n in range(len(FNameCrit)):\n",
    "        if FNameCrit[n] in list_check[n]: # expt label should match file name in same position\n",
    "            counter = counter + 1 # if so add 1 \n",
    "            if (n == 1) and (counter == 2): # if both files strings are different, append\n",
    "                CleanSubjs.append(GoodSubjs[i])\n",
    "        else: # if not, add em to the singletons ... \n",
    "            print(f'{GoodSubjs[i]} did not match file name for {FNameCrit[n]}, moving subj to proper file ind array')\n",
    "            str_nm = (np.array([GoodSubjs[i]], dtype=object))\n",
    "            SingleSubs = np.concatenate((str_nm, SingleSessSubjs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many participants **actually** completed both sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 Participants Completed AttnL and AttnR\n",
      "11 Participants did not complete both sessions\n",
      "['2651', '2652', '2653', '2654', '2655', '2657', '2658', '2659', '2661', '2664', '2665', '2666', '2667', '2668', '2669', '2670', '2671', '2672', '2674', '2677', '2695', '2696', '2697', '2728', '345202', '345215', '345216']\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(CleanSubjs)} Participants Completed AttnL and AttnR')\n",
    "print(f'{len(SingleSubs)} Participants did not complete both sessions')\n",
    "print(CleanSubjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start workbench "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data_sIn = CleanSubjs[26]\n",
    "print(f'importing subject {All_Data_sIn}s data')\n",
    "# find position of both files located\n",
    "SubjFile_Pos = [(i, x) for i, x in enumerate(FileName) if All_Data_sIn in x]\n",
    "# only get the number position\n",
    "fx_fileInds = np.array([SubjFile_Pos[0][0],SubjFile_Pos[1][0]])\n",
    "print(f'File Pos: {fx_fileInds}')\n",
    "# double check!!\n",
    "'This dict contains Attention-Induction Congruent F1 and F2 data and Atention-Induction Incongruent F1,F2 data Unprocessed',\n",
    "DuoSessionData = [f1[fx_fileInds[0]], f2[fx_fileInds[0]],f1[fx_fileInds[1]], f2[fx_fileInds[1]]]\n",
    "print(f'{FileName[fx_fileInds[0]]},{FileName[fx_fileInds[1]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Data_sIn = CleanSubjs[26]\n",
    "print(f'importing subject {All_Data_sIn}s data')\n",
    "# find position of both files located\n",
    "SubjFile_Pos = [(i, x) for i, x in enumerate(FileName) if All_Data_sIn in x]\n",
    "# only get the number position\n",
    "fx_fileInds = np.array([SubjFile_Pos[0][0],SubjFile_Pos[1][0]])\n",
    "print(f'File Pos: {fx_fileInds}')\n",
    "# double check!!\n",
    "'This dict contains Attention-Induction Congruent F1 and F2 data and Atention-Induction Incongruent F1,F2 data Unprocessed',\n",
    "# DuoSessionData = [f1[fx_fileInds[0]], f1[fx_fileInds[1]],f2[fx_fileInds[0]], f2[fx_fileInds[1]]]\n",
    "DuoSessionData = [f1[fx_fileInds[0]], f2[fx_fileInds[0]],f1[fx_fileInds[1]], f2[fx_fileInds[1]]]\n",
    "# print(f'{FileName[fx_fileInds[0]]},{FileName[fx_fileInds[1]]}')\n",
    "rca_comp = 0\n",
    "ProcessedConditionData = {} # pcd processed condition data\n",
    "diffs ={}\n",
    "\n",
    "for datamat in range(4):\n",
    "\n",
    "    ProcessedConditionData[datamat] = {}\n",
    "    diffs[datamat] ={}\n",
    "    #print(DuoSessionData[i].shape)\n",
    "    x = np.array(DuoSessionData[datamat])\n",
    "    # look at first RCA component\n",
    "    xf = x[:,rca_comp,:] # [48 x 78] 1f1 2f1 4f1 6f1\n",
    "\n",
    "    c,r = np.shape(xf) # get dims of data \n",
    "    ci = int(c/2) # real and imaginary values to divide\n",
    "    ri = int(r/2) # pre post boundries to divide \n",
    "    ComplexData = np.zeros((2,ci,r)) #real/imag x contrast/harmonic vals x trials\n",
    "\n",
    "    realData= xf[:ci,:] # real values for all trials \n",
    "    phaseData = xf[ci:,:] # imaginary values for all trials\n",
    "\n",
    "    # store data in one more consise array (reshape) \n",
    "    for i in range(2):\n",
    "        if i == 0:\n",
    "            ComplexData[0,:,:] = realData\n",
    "        else:\n",
    "            ComplexData[1,:,:] = phaseData\n",
    "\n",
    "    # get all harmonic data in a single dict\n",
    "    harmonicData = {}\n",
    "    # harmonic epochs\n",
    "    HamronicIndicies = np.arange(0,ci,6) # index to get \n",
    "\n",
    "    for j in range(4):\n",
    "        s = HamronicIndicies[j]\n",
    "        e = s+6\n",
    "        harmonicData[j] = ComplexData[:,s:e,:]  #[2 x 6 x ~80 ]\n",
    "        \n",
    "    \n",
    "    complexValues = {} # no combination of real x imaginary\n",
    "    CleanCRF = {} # contrast response functions\n",
    "\n",
    "    for l in range(4):\n",
    "        hd = harmonicData[l] # 2 x 6 x numtrials\n",
    "\n",
    "        p1 = hd[:,:,:ri] # pre  [2x6xnumtrials]\n",
    "        p2 = hd[:,:,ri:] # post \n",
    "\n",
    "        px_set = [p1,p2] # store pre post in temp array\n",
    "\n",
    "        cplxValsRec = np.zeros((2,2,6)) # pre/postx real/imaginary values x per contrast sweep incriment\n",
    "        crfValsRec = np.zeros((2,6))\n",
    "\n",
    "        for pp in range(2):\n",
    "            dIn = px_set[pp] # import pre or post data\n",
    "            for sweep in range(6): # import data for 1 contrast only \n",
    "                avgVal = dIn[:,sweep,:] # 2 x 39 vals\n",
    "                av = np.nanmean(avgVal,axis = 1)\n",
    "                crfValsRec[pp,sweep] = np.hypot(av[0],av[1])\n",
    "\n",
    "            cplxValsRec[pp,:,sweep] = av\n",
    "        #crf_diff = (crfValsRec[1,:] - crfValsRec[0,:])\n",
    "        pp_diff = (crfValsRec[1,:] - crfValsRec[0,:]) / (crfValsRec[0,:] + crfValsRec[1,:])\n",
    "\n",
    "\n",
    "        complexValues[l] = cplxValsRec\n",
    "\n",
    "        #CleanCRF[l] = crfValsRec\n",
    "\n",
    "        ProcessedConditionData[datamat][l] = crfValsRec\n",
    "\n",
    "        diffs[datamat][l] = pp_diff\n",
    "    \n",
    "    # for k in range(1):\n",
    "    #     crf = CleanCRF[k]\n",
    "    #     cdata = complexValues[k]\n",
    "\n",
    "    \n",
    "# # custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "# sns.set_theme(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n",
    "fig,pxs = plt.subplots(1,4, figsize  = (15,4), sharey = True)\n",
    "plt.suptitle(f'Subject #{All_Data_sIn}')\n",
    "pig,fxs  = plt.subplots(figsize = (5,5))\n",
    "plt.suptitle(f'Subject #{All_Data_sIn} Post Induction Response')\n",
    "\n",
    "labels = ['A-I C','A-I Control','A-I INC','~A-I']\n",
    "for p in range(4):\n",
    "    crfData = ProcessedConditionData[p][0]\n",
    "    d_diffs = diffs[p][0]\n",
    "\n",
    "    pxs[p].plot(crfData[0,:], label = f'pre {labels[p]}', color = 'black')\n",
    "    pxs[p].plot(crfData[1,:], label = f'post {labels[p]}', color = 'blue')\n",
    "    pxs[p].hlines(0,xmin = 0, xmax = 5, color = 'black',linewidth = 4)\n",
    "    pxs[p].plot(d_diffs, label = f'Net Inc.', color = 'red')\n",
    "    pxs[p].legend(loc  = 'upper left')\n",
    "\n",
    "\n",
    "    fxs.plot(d_diffs, label = f'{labels[p]}', linewidth = 8)\n",
    "    fxs.hlines(0,xmin = 0, xmax = 5, color = 'black',linewidth = 4)\n",
    "    fxs.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rca_comp = 0\n",
    "str_split = '-'\n",
    "# All_Data_sIn = SingleSessSubjs[6]\n",
    "for sIn in range(len(SingleSessSubjs)):\n",
    "    All_Data_sIn = SingleSessSubjs[sIn]\n",
    "    #SingleSessSubjs or CleanSubjs\n",
    "\n",
    "    print(f'importing subject {All_Data_sIn}s data')\n",
    "    SubjFile_Pos = [(i, x) for i, x in enumerate(FileName) if All_Data_sIn in x]\n",
    "    # only get the number position\n",
    "    'this code chunk deals with incomplete data by providing and creating  matrix of nans'\n",
    "    if (len(SubjFile_Pos)) == 1:\n",
    "        'make emoty df for missing values to get pretty plots still '\n",
    "        emptyMat_df1 = np.zeros((48,4,78))\n",
    "        emptyMat_df2 = np.zeros_like((emptyMat_df1))\n",
    "        emptyMat_df1[:] - np.nan\n",
    "        emptyMat_df2[:] - np.nan\n",
    "\n",
    "        print(f'only 1 file found')\n",
    "        'locate file position only - contains file name too'\n",
    "        fx_fileInds = np.array([SubjFile_Pos[0][0]])\n",
    "        SessionCond = SubjFile_Pos[0][1]\n",
    "        #print(SessionCond)\n",
    "        'determine what data is missing (attnL/R) and place data in correct format'\n",
    "        Expt = SessionCond.split(str_split)[1]\n",
    "        if Expt == 'attnL':\n",
    "            #'import first data poitions and set latter conditions as missing data'\n",
    "            print(f'AttnL File Pos: {fx_fileInds}, Exp Session: {SessionCond}')\n",
    "\n",
    "            DuoSessionData = [f1[fx_fileInds[0]], f2[fx_fileInds[0]],emptyMat_df1,emptyMat_df2]\n",
    "\n",
    "        elif Expt == 'attnR':\n",
    "            #'import first data poitions as missing data and import last data positions as missing data'\n",
    "            print(f'AttnR File Pos: {fx_fileInds}, Exp Session: {SessionCond}')\n",
    "\n",
    "            DuoSessionData = [emptyMat_df1,emptyMat_df2,f1[fx_fileInds[0]], f2[fx_fileInds[0]]]\n",
    "\n",
    "\n",
    "    # 'If subject completed both data files, divvy data'\n",
    "    else:\n",
    "        print(f'2 files found')\n",
    "        fx_fileInds = np.array([SubjFile_Pos[0][0],SubjFile_Pos[1][0]])\n",
    "        print(f'File Pos: {fx_fileInds}')\n",
    "        'This dict contains Attention-Induction Congruent F1 and F2 data and Atention-Induction Incongruent F1,F2 data Unprocessed'\n",
    "\n",
    "        DuoSessionData = [f1[fx_fileInds[0]], f2[fx_fileInds[0]],f1[fx_fileInds[1]], f2[fx_fileInds[1]]]\n",
    "        #print(f'{FileName[fx_fileInds[0]]},{FileName[fx_fileInds[1]]}')\n",
    "\n",
    "\n",
    "    ProcessedConditionData = {} # pcd processed condition data\n",
    "    diffs ={}\n",
    "    for datamat in range(4):\n",
    "\n",
    "        ProcessedConditionData[datamat] = {}\n",
    "        diffs[datamat] ={}\n",
    "        #print(DuoSessionData[i].shape)\n",
    "        x = np.array(DuoSessionData[datamat])\n",
    "        # look at first RCA component\n",
    "        xf = x[:,rca_comp,:] # [48 x 78] 1f1 2f1 4f1 6f1\n",
    "\n",
    "        c,r = np.shape(xf) # get dims of data \n",
    "        ci = int(c/2) # real and imaginary values to divide\n",
    "        ri = int(r/2) # pre post boundries to divide \n",
    "        ComplexData = np.zeros((2,ci,r)) #real/imag x contrast/harmonic vals x trials\n",
    "\n",
    "        realData= xf[:ci,:] # real values for all trials \n",
    "        phaseData = xf[ci:,:] # imaginary values for all trials\n",
    "\n",
    "        # store data in one more consise array (reshape) \n",
    "        for i in range(2):\n",
    "            if i == 0:\n",
    "                ComplexData[0,:,:] = realData\n",
    "            else:\n",
    "                ComplexData[1,:,:] = phaseData\n",
    "\n",
    "        # get all harmonic data in a single dict\n",
    "        harmonicData = {}\n",
    "        # harmonic epochs\n",
    "        HamronicIndicies = np.arange(0,ci,6) # index to get \n",
    "\n",
    "        for j in range(4):\n",
    "            s = HamronicIndicies[j]\n",
    "            e = s+6\n",
    "            harmonicData[j] = ComplexData[:,s:e,:]  #[2 x 6 x ~80 ]\n",
    "            \n",
    "        \n",
    "        complexValues = {} # no combination of real x imaginary\n",
    "        CleanCRF = {} # contrast response functions\n",
    "\n",
    "        for l in range(4):\n",
    "            hd = harmonicData[l] # 2 x 6 x numtrials\n",
    "\n",
    "            p1 = hd[:,:,:ri] # pre  [2x6xnumtrials]\n",
    "            p2 = hd[:,:,ri:] # post \n",
    "\n",
    "            px_set = [p1,p2] # store pre post in temp array\n",
    "\n",
    "            cplxValsRec = np.zeros((2,2,6)) # pre/postx real/imaginary values x per contrast sweep incriment\n",
    "            crfValsRec = np.zeros((2,6))\n",
    "\n",
    "            for pp in range(2):\n",
    "                dIn = px_set[pp] # import pre or post data\n",
    "                for sweep in range(6): # import data for 1 contrast only \n",
    "                    avgVal = dIn[:,sweep,:] # 2 x 39 vals\n",
    "                    av = np.nanmean(avgVal,axis = 1)\n",
    "                    crfValsRec[pp,sweep] = np.hypot(av[0],av[1])\n",
    "\n",
    "                cplxValsRec[pp,:,sweep] = av\n",
    "            #crf_diff = (crfValsRec[1,:] - crfValsRec[0,:])\n",
    "            pp_diff = (crfValsRec[1,:] - crfValsRec[0,:]) / (crfValsRec[0,:] + crfValsRec[1,:])\n",
    "\n",
    "\n",
    "            complexValues[l] = cplxValsRec\n",
    "\n",
    "            #CleanCRF[l] = crfValsRec\n",
    "\n",
    "            ProcessedConditionData[datamat][l] = crfValsRec\n",
    "\n",
    "            diffs[datamat][l] = pp_diff\n",
    "        \n",
    "        # for k in range(1):\n",
    "    fig,pxs = plt.subplots(1,4, figsize  = (15,4), sharey = True)\n",
    "    plt.suptitle(f'Subject #{All_Data_sIn}')\n",
    "    pig,fxs  = plt.subplots(figsize = (5,5))\n",
    "    plt.suptitle(f'Subject #{All_Data_sIn} Post Induction Response')\n",
    "\n",
    "    labels = ['A-I C','A-I Control','A-I INC','~A-I']\n",
    "    for p in range(4):\n",
    "        crfData = ProcessedConditionData[p][0]\n",
    "        d_diffs = diffs[p][0]\n",
    "\n",
    "        pxs[p].plot(crfData[0,:], label = f'pre {labels[p]}', color = 'black')\n",
    "        pxs[p].plot(crfData[1,:], label = f'post {labels[p]}', color = 'blue')\n",
    "        pxs[p].hlines(0,xmin = 0, xmax = 5, color = 'black',linewidth = 4)\n",
    "        pxs[p].plot(d_diffs, label = f'Net Inc.', color = 'red')\n",
    "        pxs[p].legend(loc  = 'upper left', fontsize = 8)\n",
    "\n",
    "        fxs.plot(d_diffs, label = f'{labels[p]}', linewidth = 8)\n",
    "        fxs.hlines(0,xmin = 0, xmax = 5, color = 'black',linewidth = 4)\n",
    "        fxs.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check!!\n",
    "'This dict contains Attention-Induction Congruent F1 and F2 data and Atention-Induction Incongruent F1,F2 data Unprocessed',\n",
    "DuoSessionData = [f1[fx_fileInds[0]], f1[fx_fileInds[1]],f2[fx_fileInds[0]], f2[fx_fileInds[1]]]\n",
    "print(f'{FileName[fx_fileInds[0]]},{FileName[fx_fileInds[1]]}')\n",
    "# rca_comp = 0\n",
    "ProcessedConditionData = {} # pcd processed condition data\n",
    "diffs ={}\n",
    "\n",
    "for datamat in range(4):\n",
    "\n",
    "    ProcessedConditionData[datamat] = {}\n",
    "    diffs[datamat] ={}\n",
    "    #print(DuoSessionData[i].shape)\n",
    "    x = np.array(DuoSessionData[datamat])\n",
    "    # look at first RCA component\n",
    "    xf = x[:,rca_comp,:] # [48 x 78] 1f1 2f1 4f1 6f1\n",
    "\n",
    "    c,r = np.shape(xf) # get dims of data \n",
    "    ci = int(c/2) # real and imaginary values to divide\n",
    "    ri = int(r/2) # pre post boundries to divide \n",
    "    ComplexData = np.zeros((2,ci,r)) #real/imag x contrast/harmonic vals x trials\n",
    "\n",
    "    realData= xf[:ci,:] # real values for all trials \n",
    "    phaseData = xf[ci:,:] # imaginary values for all trials\n",
    "\n",
    "    # store data in one more consise array (reshape) \n",
    "    for i in range(2):\n",
    "        if i == 0:\n",
    "            ComplexData[0,:,:] = realData\n",
    "        else:\n",
    "            ComplexData[1,:,:] = phaseData\n",
    "\n",
    "    # get all harmonic data in a single dict\n",
    "    harmonicData = {}\n",
    "    # harmonic epochs\n",
    "    HamronicIndicies = np.arange(0,ci,6) # index to get \n",
    "\n",
    "    for j in range(4):\n",
    "        s = HamronicIndicies[j]\n",
    "        e = s+6\n",
    "        harmonicData[j] = ComplexData[:,s:e,:]  #[2 x 6 x ~80 ]\n",
    "        \n",
    "    \n",
    "    complexValues = {} # no combination of real x imaginary\n",
    "    CleanCRF = {} # contrast response functions\n",
    "\n",
    "    for l in range(4):\n",
    "        hd = harmonicData[l] # 2 x 6 x numtrials\n",
    "\n",
    "        p1 = hd[:,:,:ri] # pre  [2x6xnumtrials]\n",
    "        p2 = hd[:,:,ri:] # post \n",
    "\n",
    "        px_set = [p1,p2] # store pre post in temp array\n",
    "\n",
    "        cplxValsRec = np.zeros((2,2,6)) # pre/postx real/imaginary values x per contrast sweep incriment\n",
    "        crfValsRec = np.zeros((2,6))\n",
    "\n",
    "        for pp in range(2):\n",
    "            dIn = px_set[pp] # import pre or post data\n",
    "            for sweep in range(6): # import data for 1 contrast only \n",
    "                avgVal = dIn[:,sweep,:] # 2 x 39 vals\n",
    "                av = np.nanmean(avgVal,axis = 1)\n",
    "                crfValsRec[pp,sweep] = np.hypot(av[0],av[1])\n",
    "\n",
    "            cplxValsRec[pp,:,sweep] = av\n",
    "        #crf_diff = (crfValsRec[1,:] - crfValsRec[0,:])\n",
    "        pp_diff = (crfValsRec[1,:] - crfValsRec[0,:]) / (crfValsRec[0,:] + crfValsRec[1,:])\n",
    "\n",
    "\n",
    "        complexValues[l] = cplxValsRec\n",
    "\n",
    "        #CleanCRF[l] = crfValsRec\n",
    "\n",
    "        ProcessedConditionData[datamat][l] = crfValsRec\n",
    "\n",
    "        diffs[datamat][l] = pp_diff\n",
    "    \n",
    "    # for k in range(1):\n",
    "    #     crf = CleanCRF[k]\n",
    "    #     cdata = complexValues[k]\n",
    "\n",
    "    \n",
    "# # custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "# sns.set_theme(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n",
    "fig,pxs = plt.subplots(1,4, figsize  = (15,4), sharey = True)\n",
    "plt.suptitle(f'Subject #{All_Data_sIn}')\n",
    "pig,fxs  = plt.subplots(figsize = (5,5))\n",
    "plt.suptitle(f'Subject #{All_Data_sIn} Post Induction Response')\n",
    "\n",
    "labels = ['A-I C','A-I Control','A-I INC','~A-I']\n",
    "for p in range(4):\n",
    "    crfData = ProcessedConditionData[p][0]\n",
    "    d_diffs = diffs[p][0]\n",
    "\n",
    "    pxs[p].plot(crfData[0,:], label = f'pre {labels[p]}', color = 'black')\n",
    "    pxs[p].plot(crfData[1,:], label = f'post {labels[p]}', color = 'blue')\n",
    "    pxs[p].hlines(0,xmin = 0, xmax = 5, color = 'black',linewidth = 4)\n",
    "    pxs[p].plot(d_diffs, label = f'Net Inc.', color = 'red')\n",
    "    pxs[p].legend(loc  = 'upper left')\n",
    "\n",
    "\n",
    "    fxs.plot(d_diffs, label = f'{labels[p]}', linewidth = 8)\n",
    "    fxs.hlines(0,xmin = 0, xmax = 5, color = 'black',linewidth = 4)\n",
    "    fxs.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Data Files as AttnL and AttnR in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNameCrit = ['attnL', 'attnR']\n",
    "FilePos = np.zeros((len(CleanSubjs), 2))\n",
    "\n",
    "for name in range(len(CleanSubjs)):\n",
    "    yIn = CleanSubjs[name] # import single subject who completed 2 sessions \n",
    "    all_files_avil = [x for x in FileName if yIn in x] # list\n",
    "    all_files_avil = np.sort(all_files_avil) # might not be necessary but jic ...\n",
    "    pos = [loc for loc, file in enumerate(FileName) if file in all_files_avil] # index position of files if files match attnL and attnR\n",
    "    FilePos[name,:] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SessNumIn = 8 # enter subj number\n",
    "NumHarms = 4 # 2f2 4f1 6f2 8f1\n",
    "rca_comp = 0 # enter rca component \n",
    "# set string to array \\\n",
    "for SessNumIn in range(len(f1)):\n",
    "    x = np.array(f2[SessNumIn])\n",
    "    # look at first RCA component\n",
    "    xf = x[:,rca_comp,:] # [48 x 78] 1f1 2f1 4f1 6f1\n",
    "    c,r = np.shape(xf) # get dims of data \n",
    "    ci = int(c/2) # real and imaginary values to divide\n",
    "    ri = int(r/2) # pre post boundries to divide \n",
    "    ComplexData = np.zeros((2,ci,r)) #pre post x  real/imag x trials\n",
    "\n",
    "    realData= xf[:ci,:] # real values for all trials \n",
    "    phaseData = xf[ci:,:] # imaginary values for all trials\n",
    "\n",
    "    # store data in one more consise array (reshape)\n",
    "    for i in range(2):\n",
    "        if i == 0:\n",
    "            ComplexData[i,:,:] = realData\n",
    "        else:\n",
    "            ComplexData[i,:,:] = phaseData\n",
    "\n",
    "    # get all harmonic data in a single dict\n",
    "    harmonicData = {}\n",
    "    HamronicIndicies = np.arange(0,ci,6) # index to get \n",
    "    for j in range(4):\n",
    "        s = HamronicIndicies[j]\n",
    "        e = s+6\n",
    "        harmonicData[j] = ComplexData[:,s:e,:] \n",
    "    complexValues = {}\n",
    "    CleanCRF = {}\n",
    "    for l in range(4):\n",
    "        hd = harmonicData[l] # 2 x 6 x numtrials\n",
    "\n",
    "        p1 = hd[:,:,:ri] # pre \n",
    "        p2 = hd[:,:,ri:] # post \n",
    "\n",
    "        px_set = [p1,p2]\n",
    "\n",
    "        cplxValsRec = np.zeros((2,2,6)) # pre/postx real/imaginary values x per contrast sweep incriment\n",
    "        crfValsRec = np.zeros((2,6))\n",
    "\n",
    "        for pp in range(2):\n",
    "            dIn = px_set[pp] # import pre or post data\n",
    "            for sweep in range(6): # import data for 1 contrast only \n",
    "                avgVal = dIn[:,sweep,:] # 2 x 39 vals\n",
    "                av = np.nanmean(avgVal,axis = 1)\n",
    "                crfValsRec[pp,sweep] = np.hypot(av[0],av[1])\n",
    "            cplxValsRec[pp,:,sweep] = av\n",
    "        crf_diff = (crfValsRec[1,:] - crfValsRec[0,:])\n",
    "\n",
    "        complexValues[l] = cplxValsRec\n",
    "        CleanCRF[l] = crfValsRec\n",
    "    for k in range(1):\n",
    "        crf = CleanCRF[k]\n",
    "        cdata = complexValues[k]\n",
    "\n",
    "        fig,axs = plt.subplots(1,2,figsize =(6,3),sharey = False)\n",
    "        axs[0].plot(crf[0,:], label = 'pre', color = 'black')\n",
    "        axs[0].plot(crf[1,:], label = 'post', color = 'blue')\n",
    "        axs[0].plot(crf[1,:] - crf[0,:], label = 'post increase  ', color = 'red')\n",
    "        axs[0].hlines(0, xmin = 0, xmax = 5, color = 'black', linestyles = 'dashed')\n",
    "        axs[0].legend(loc = 'upper left', fontsize = 6)\n",
    "\n",
    "        axs[1].scatter(cdata[0,0,:],cplxValsRec[0,1,:], label = 'pre complexs', linewidth  = 6, color = 'black')\n",
    "        axs[1].scatter(cdata[1,0,:],cplxValsRec[1,1,:], label = 'post complexs',linewidth  = 2 )\n",
    "        axs[1].hlines(0, xmin = -2, xmax = 2, color = 'black', linestyles = 'dashed')\n",
    "        axs[1].vlines(0, ymin = -1, ymax = 1, color = 'black', linestyles = 'dashed')\n",
    "        axs[1].legend(fontsize= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a big for loop to save all this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AttnL =  {'F1': [], 'F2': []} \n",
    "AttnR =  {'F1': [], 'F2': []} \n",
    "#AttnX =  {'F1': [], 'F2': []} \n",
    "\n",
    "# run a different file we imported (2 F1 and F2 filtered data) to save all in the same file\n",
    "for iter in range(NumFiles):\n",
    "    for ind in range(len(CleanSubjs)):\n",
    "        attnL_FilePos = int(FilePos[ind,0])\n",
    "        attnR_FilePos = int(FilePos[ind,1])\n",
    "        if iter == 0:\n",
    "            data = f1\n",
    "            AttnL['F1'].append(data[attnL_FilePos])  # Append value to list in 'F1' key\n",
    "            AttnR['F1'].append(data[attnR_FilePos])  # Append value to list in 'F1' key\n",
    "        elif iter == 1:\n",
    "            data = f2\n",
    "            AttnL['F2'].append(data[attnL_FilePos])  # Append value to list in 'F2' key\n",
    "            AttnR['F2'].append(data[attnR_FilePos])  # Append value to list in 'F2' key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Saving Subjects data who only completed 1 session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SingleFiles_arr = []\n",
    "for file in range(len(SingleSubs)):\n",
    "    subIn = SingleSubs[file] # import one file at a time\n",
    "    files_avil = [x for x in FileName if subIn in x]\n",
    "    SingleFiles_arr.append(files_avil)\n",
    "\n",
    "SinglefileNames = np.array(list(chain(*SingleFiles_arr))) #all single session names flattened\n",
    "print(SinglefileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_catch_L = 'attnL'\n",
    "string_catch_R = 'attnR'\n",
    "single_sess_ind = np.zeros(len(SinglefileNames)) # size of single sessions available\n",
    "SingleSessSubName = [] # save subject name \n",
    "single_sess_pos = [] # find and store the data index \n",
    "\n",
    "for sInFName in range(len(SinglefileNames)):\n",
    "    # determine whether its attnr or attnL\n",
    "    fIn = SinglefileNames[sInFName]\n",
    "    #find the postion of file in the data to organize later\n",
    "    pos = [posi for posi, file in enumerate(FileName) if file in fIn]\n",
    "    single_sess_pos.append(pos)\n",
    "\n",
    "    x = fIn.split(string_ind)[1]\n",
    "    y = fIn.split(string_ind)[0]\n",
    "    SingleSessSubName.append(y)\n",
    "    # make array to findex what files are attnL and attnR\n",
    "    if string_catch_L in x:\n",
    "        single_sess_ind[sInFName] = 1 # attnL ind == 1\n",
    "    elif string_catch_R in x:\n",
    "        single_sess_ind[sInFName] = 0 # attnL ind == 0\n",
    "\n",
    "single_sess_pos = np.array(single_sess_pos)\n",
    "# print(single_sess_pos)\n",
    "# print(single_sess_ind)\n",
    "# print(SingleSessSubName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index and save singles sessions as a seperate dict to export in pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sess_AttnL =  {'F1': [], 'F2': []} \n",
    "single_sess_AttnR =  {'F1': [], 'F2': []} \n",
    "l_subs = []\n",
    "r_subs = []\n",
    "\n",
    "for file_op in range(NumFiles):\n",
    "    for oneSess in range(len(SinglefileNames)):\n",
    "        sIn = SingleSessSubName[oneSess] # single sub names\n",
    "        AttnXCond = single_sess_ind[oneSess] # condtion they did\n",
    "        DataPos = single_sess_pos[oneSess] # position of data file is \n",
    "        DataPos = int(DataPos[0]) \n",
    "\n",
    "        if file_op == 0:\n",
    "            dataIn = f1 # switch files 2 combine them\n",
    "            if AttnXCond == 1:\n",
    "                single_sess_AttnL['F1'].append(dataIn[DataPos]) # save data in this dict\n",
    "                l_subs.append(sIn) # save subject name in this dict\n",
    "            else:\n",
    "                single_sess_AttnR['F1'].append(dataIn[DataPos])\n",
    "                r_subs.append(sIn)\n",
    "\n",
    "        elif file_op == 1:\n",
    "            dataIn = f2 # switch files 2 combine them\n",
    "            if AttnXCond == 1:\n",
    "                single_sess_AttnL['F2'].append(dataIn[DataPos])\n",
    "            else:\n",
    "                single_sess_AttnR['F2'].append(dataIn[DataPos])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SingleSessDataOut = {}\n",
    "\n",
    "SingleSessDataOut[0] = single_sess_AttnL['F1']\n",
    "SingleSessDataOut[1] = single_sess_AttnL['F2']\n",
    "SingleSessDataOut[2] = single_sess_AttnR['F1']\n",
    "SingleSessDataOut[3] = single_sess_AttnR['F2']\n",
    "\n",
    "SingleSessDataOut['AttnLSubNames'] = np.array(l_subs)\n",
    "SingleSessDataOut['AttnRSubNames'] = np.array(r_subs)\n",
    "#SingleSessDataOut['DataNotes'] = ['keys: 0&1 attnL[f1/f2] and 2&3 attR[f1/f2], single session data']\n",
    "print(SingleSessDataOut.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SessDataOut = {}\n",
    "\n",
    "SessDataOut[0] = AttnL['F1']\n",
    "SessDataOut[1] = AttnL['F2']\n",
    "SessDataOut[2] = AttnR['F1']\n",
    "SessDataOut[3] = AttnR['F2']\n",
    "\n",
    "SessDataOut['FullSessSubjNames'] = CleanSubjs\n",
    "SessDataOut['DataNotes'] = ['keys: 0&1 attnL[f1/f2] and 2&3 attR[f1/f2]']\n",
    "print(SessDataOut.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data into .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOut = {}\n",
    "\n",
    "dataOut[0] = SessDataOut\n",
    "dataOut[1] = SingleSessDataOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile = 'y'\n",
    "\n",
    "if saveFile == 'y':\n",
    " with open(NewFileNPath, 'wb') as file:\n",
    "    pkl.dump(dataOut, file, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    print('Sorted Data Saved! :))')\n",
    "else:\n",
    "    print('Did Not Save File! Change file name before switching to y!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
